{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, math\n",
    "import os.path as osp\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from pprint import pprint\n",
    "import pickle, json, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacred\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sacred import SETTINGS\n",
    "from sacred.utils import apply_backspaces_and_linefeeds\n",
    "from torch.backends import cudnn\n",
    "from torch.optim import SGD, Adam, AdamW, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.backends import cudnn\n",
    "# from visdom_logger import VisdomLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, BatchSampler\n",
    "from typing import NamedTuple, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.matcher import MatchERT, PosMatchERT\n",
    "from models.ingredient import model_ingredient, get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ingredient import model_ingredient, get_model\n",
    "from utils import state_dict_to_cpu, num_of_trainable_params\n",
    "from utils import pickle_load, pickle_save\n",
    "#from utils.data.utils import TripletSampler\n",
    "from utils import BinaryCrossEntropyWithLogits\n",
    "from utils.data.dataset_ingredient import data_ingredient, get_loaders\n",
    "from utils.training import train_one_epoch, evaluate_viquae\n",
    "from utils.metrics import AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import pickle_load\n",
    "from sacred import Experiment\n",
    "from utils.data.dataset_ingredient import data_ingredient, get_loaders\n",
    "from utils.data.dataset import FeatureDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sacred import Experiment\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = sacred.Experiment('RRT Training', ingredients=[data_ingredient, model_ingredient], interactive=True)\n",
    "# Filter backspaces and linefeeds\n",
    "SETTINGS.CAPTURE_MODE = 'sys'\n",
    "ex.captured_out_filter = apply_backspaces_and_linefeeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sets(desc_name, \n",
    "        train_data_dir, test_data_dir, train_txt, \n",
    "        test_txt, train_gnd_file,  test_gnd_file,\n",
    "        max_sequence_len, split_char, prefixed):\n",
    "    ####################################################################################################################################\n",
    "    train_gnd_file = prefixed+'_'+train_gnd_file if prefixed is not None and train_gnd_file is not None else train_gnd_file\n",
    "    test_gnd_file  = prefixed+'_'+test_gnd_file  if prefixed is not None and test_gnd_file  is not None else test_gnd_file\n",
    "    \n",
    "    if len(train_txt) == 2:\n",
    "        \n",
    "        train_gnd_data  = None if train_gnd_file is None else pickle_load(osp.join(train_data_dir, train_gnd_file))\n",
    "        train_lines_txt = train_txt[1] if prefixed is None else prefixed+'_'+train_txt[1]\n",
    "        train_lines     = read_file(osp.join(train_data_dir, train_lines_txt))\n",
    "        train_q_lines_txt = train_txt[0] if prefixed is None else prefixed+'_'+train_txt[0]\n",
    "        train_q_lines   = read_file(osp.join(train_data_dir, train_q_lines_txt))\n",
    "        train_samples   = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in train_lines]\n",
    "        train_q_samples = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in train_q_lines]\n",
    "        train_set       = FeatureDataset(train_data_dir, train_samples,   desc_name, max_sequence_len, gnd_data=train_gnd_data)\n",
    "        query_train_set = FeatureDataset(train_data_dir, train_q_samples, desc_name, max_sequence_len, gnd_data=train_gnd_data)\n",
    "    else:\n",
    "        train_gnd_data  = None if train_gnd_file is None else pickle_load(osp.join(train_data_dir, train_gnd_file))\n",
    "        train_lines_txt = train_txt if prefixed is None else prefixed+'_'+train_txt\n",
    "        train_lines     = read_file(osp.join(train_data_dir, train_lines_txt))\n",
    "        train_samples   = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in train_lines]\n",
    "        train_set       = FeatureDataset(train_data_dir, train_samples, desc_name, max_sequence_len, gnd_data=train_gnd_data)\n",
    "        query_train_set = FeatureDataset(train_data_dir, train_samples, desc_name, max_sequence_len, gnd_data=train_gnd_data)\n",
    "        ####################################################################################################################################\n",
    "    test_gnd_data   = None if test_gnd_file is None else pickle_load(osp.join(test_data_dir, test_gnd_file))\n",
    "    query_lines_txt = test_txt[0] if prefixed is None else prefixed+'_'+test_txt[0]\n",
    "    query_lines     = read_file(osp.join(test_data_dir, query_lines_txt))\n",
    "    gallery_lines_txt = test_txt[1] if prefixed is None else prefixed+'_'+test_txt[1]\n",
    "    gallery_lines   = read_file(osp.join(test_data_dir, gallery_lines_txt))\n",
    "    query_samples   = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in query_lines]\n",
    "    gallery_samples = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in gallery_lines]\n",
    "    gallery_set     = FeatureDataset(test_data_dir, gallery_samples, desc_name, max_sequence_len)\n",
    "    query_set       = FeatureDataset(test_data_dir, query_samples,   desc_name, max_sequence_len, gnd_data=test_gnd_data)\n",
    "        \n",
    "    return (train_set, query_train_set), (query_set, gallery_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sets_limit_1(desc_name, \n",
    "        train_data_dir, test_data_dir, train_txt, \n",
    "        test_txt, train_gnd_file,  test_gnd_file,\n",
    "        max_sequence_len, split_char, prefixed):\n",
    "    ####################################################################################################################################\n",
    "    train_gnd_file = prefixed+'_'+train_gnd_file if prefixed is not None and train_gnd_file is not None else train_gnd_file\n",
    "    test_gnd_file  = prefixed+'_'+test_gnd_file  if prefixed is not None and test_gnd_file  is not None else test_gnd_file\n",
    "    \n",
    "    if len(train_txt) == 2:\n",
    "        \n",
    "        train_gnd_data  = None if train_gnd_file is None else pickle_load(osp.join(train_data_dir, train_gnd_file))\n",
    "        train_lines_txt = train_txt[1] if prefixed is None else prefixed+'_'+train_txt[1]\n",
    "        train_lines     = read_file(osp.join(train_data_dir, train_lines_txt))[0]\n",
    "        train_q_lines_txt = train_txt[0] if prefixed is None else prefixed+'_'+train_txt[0]\n",
    "        train_q_lines   = read_file(osp.join(train_data_dir, train_q_lines_txt))\n",
    "        train_samples   = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in train_lines]\n",
    "        train_q_samples = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in train_q_lines]\n",
    "        train_set       = FeatureDataset(train_data_dir, train_samples,   desc_name, max_sequence_len, gnd_data=train_gnd_data)\n",
    "        query_train_set = FeatureDataset(train_data_dir, train_q_samples, desc_name, max_sequence_len, gnd_data=train_gnd_data)\n",
    "    else:\n",
    "        train_gnd_data  = None if train_gnd_file is None else pickle_load(osp.join(train_data_dir, train_gnd_file))\n",
    "        train_lines_txt = train_txt if prefixed is None else prefixed+'_'+train_txt\n",
    "        train_lines     = read_file(osp.join(train_data_dir, train_lines_txt))\n",
    "        train_samples   = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in train_lines]\n",
    "        train_set       = FeatureDataset(train_data_dir, train_samples, desc_name, max_sequence_len, gnd_data=train_gnd_data)\n",
    "        query_train_set = FeatureDataset(train_data_dir, train_samples, desc_name, max_sequence_len, gnd_data=train_gnd_data)\n",
    "        ####################################################################################################################################\n",
    "    test_gnd_data   = None if test_gnd_file is None else pickle_load(osp.join(test_data_dir, test_gnd_file))\n",
    "    query_lines_txt = test_txt[0] if prefixed is None else prefixed+'_'+test_txt[0]\n",
    "    query_lines     = read_file(osp.join(test_data_dir, query_lines_txt))\n",
    "    gallery_lines_txt = test_txt[1] if prefixed is None else prefixed+'_'+test_txt[1]\n",
    "    gallery_lines   = read_file(osp.join(test_data_dir, gallery_lines_txt))\n",
    "    query_samples   = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in query_lines]\n",
    "    gallery_samples = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in gallery_lines]\n",
    "    gallery_set     = FeatureDataset(test_data_dir, gallery_samples, desc_name, max_sequence_len)\n",
    "    query_set       = FeatureDataset(test_data_dir, query_samples,   desc_name, max_sequence_len, gnd_data=test_gnd_data)\n",
    "        \n",
    "    return (train_set, query_train_set), (query_set, gallery_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(desc_name, train_data_dir, \n",
    "    batch_size, test_batch_size, \n",
    "    num_workers, pin_memory, \n",
    "    sampler, recalls, set_name, \n",
    "    eval_set_name, train_gnd_file,\n",
    "    prefixed, num_candidates=100):\n",
    "\n",
    "    (train_set, query_train_set), (query_set, gallery_set) = get_sets(desc_name, \n",
    "        train_data_dir=train_data_dir,\n",
    "        test_data_dir=train_data_dir,\n",
    "        train_txt=set_name+'_query.txt',\n",
    "        test_txt=(set_name+'_query.txt', set_name+'_selection.txt'),\n",
    "        test_gnd_file=test_gnd_file, \n",
    "        train_gnd_file=train_gnd_file,\n",
    "        split_char=split_char,\n",
    "        prefixed=prefixed,\n",
    "        max_sequence_len=500)\n",
    "\n",
    "    if sampler == 'random':\n",
    "        train_sampler = BatchSampler(RandomSampler(train_set), batch_size=batch_size, drop_last=False)\n",
    "    elif sampler == 'triplet':\n",
    "        nn_inds_path = set_name+'_nn_inds_%s.pkl'%desc_name if prefixed is None else prefixed+'_'+set_name + '_nn_inds_%s.pkl'%desc_name\n",
    "        train_nn_inds = osp.join(train_data_dir, nn_inds_path)\n",
    "        gnd_data = train_set.gnd_data['gnd']\n",
    "        train_sampler = TripletSampler(query_train_set.targets, batch_size, train_nn_inds, num_candidates, gnd_data)\n",
    "    else:\n",
    "        raise ValueError('Invalid choice of sampler ({}).'.format(sampler))\n",
    "    train_loader = DataLoader(train_set, batch_sampler=train_sampler, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    query_train_loader = DataLoader(query_train_set, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)\n",
    "        \n",
    "    query_loader   = DataLoader(query_set, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    gallery_loader = DataLoader(gallery_set, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    return MetricLoaders(train=train_loader, query_train=query_train_loader, query=query_loader, gallery=gallery_loader, num_classes=len(train_set.categories),set_name=set_name,eval_set_name=eval_set_name,prefixed=prefixed), recalls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders_limit_1(desc_name, train_data_dir, \n",
    "    batch_size, test_batch_size, \n",
    "    num_workers, pin_memory, \n",
    "    sampler, recalls, set_name, \n",
    "    eval_set_name, train_gnd_file,\n",
    "    prefixed, num_candidates=100):\n",
    "\n",
    "    (train_set, query_train_set), (query_set, gallery_set) = get_sets_limit_1(desc_name, \n",
    "        train_data_dir=train_data_dir,\n",
    "        test_data_dir=train_data_dir,\n",
    "        train_txt=set_name+'_query.txt',\n",
    "        test_txt=(set_name+'_query.txt', set_name+'_selection.txt'),\n",
    "        test_gnd_file=test_gnd_file, \n",
    "        train_gnd_file=train_gnd_file,\n",
    "        split_char=split_char,\n",
    "        prefixed=prefixed,\n",
    "        max_sequence_len=500)\n",
    "\n",
    "    if sampler == 'random':\n",
    "        train_sampler = BatchSampler(RandomSampler(train_set), batch_size=batch_size, drop_last=False)\n",
    "    elif sampler == 'triplet':\n",
    "        nn_inds_path = set_name+'_nn_inds_%s.pkl'%desc_name if prefixed is None else prefixed+'_'+set_name + '_nn_inds_%s.pkl'%desc_name\n",
    "        train_nn_inds = osp.join(train_data_dir, nn_inds_path)\n",
    "        gnd_data = train_set.gnd_data['gnd']\n",
    "        train_sampler = TripletSampler(query_train_set.targets, batch_size, train_nn_inds, num_candidates, gnd_data)\n",
    "    else:\n",
    "        raise ValueError('Invalid choice of sampler ({}).'.format(sampler))\n",
    "    train_loader = DataLoader(train_set, batch_sampler=train_sampler, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    query_train_loader = DataLoader(query_train_set, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)\n",
    "        \n",
    "    query_loader   = DataLoader(query_set, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    gallery_loader = DataLoader(gallery_set, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    return MetricLoaders(train=train_loader, query_train=query_train_loader, query=query_loader, gallery=gallery_loader, num_classes=len(train_set.categories),set_name=set_name,eval_set_name=eval_set_name,prefixed=prefixed), recalls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLoaders(NamedTuple):\n",
    "    train: DataLoader\n",
    "    num_classes: int\n",
    "    query: DataLoader\n",
    "    query_train: DataLoader\n",
    "    prefixed: str = None\n",
    "    set_name: str = ''\n",
    "    eval_set_name: str = ''\n",
    "    gallery: Optional[DataLoader] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "lr = 0.0001\n",
    "momentum = 0.\n",
    "nesterov = False\n",
    "weight_decay = 5e-4\n",
    "optim = 'adamw'\n",
    "scheduler = 'multistep'\n",
    "max_norm = 0.0\n",
    "seed = 0\n",
    "\n",
    "visdom_port = None\n",
    "visdom_freq = 100\n",
    "cpu = False  # Force training on CPU\n",
    "cudnn_flag = 'benchmark'\n",
    "temp_file = 'temp'\n",
    "\n",
    "no_bias_decay = False\n",
    "loss = 'bce'\n",
    "scheduler_tau = [16, 18]\n",
    "scheduler_gamma = 0.1\n",
    "\n",
    "temp_dir = osp.join('outputs', 'temp')\n",
    "\n",
    "resume = None\n",
    "resume = '/mnt/beegfs/home/smessoud/RerankingTransformer/RRT_GLD/rrt_gld_ckpts/r50_gldv2.pt'\n",
    "classifier = False\n",
    "transformer = False\n",
    "last_layers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " False,\n",
       " 'benchmark',\n",
       " None,\n",
       " 100,\n",
       " 'outputs/temp',\n",
       " 0,\n",
       " False,\n",
       " 0.0,\n",
       " '/mnt/beegfs/home/smessoud/RerankingTransformer/RRT_GLD/rrt_gld_ckpts/r50_gldv2.pt')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs, cpu, cudnn_flag, visdom_port, visdom_freq, temp_dir, seed, no_bias_decay, max_norm, resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various Training Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_scheduler(parameters, optim, loader_length, epochs, lr, momentum, nesterov, weight_decay, scheduler, scheduler_tau, scheduler_gamma, lr_step=None):\n",
    "    if optim == 'sgd':\n",
    "        optimizer = SGD(parameters, lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=True if nesterov and momentum else False)\n",
    "    elif optim == 'adam':\n",
    "        optimizer = Adam(parameters, lr=lr, weight_decay=weight_decay) \n",
    "    else:\n",
    "        optimizer = AdamW(parameters, lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    if epochs == 0:\n",
    "        scheduler = None\n",
    "        update_per_iteration = None\n",
    "    elif scheduler == 'cos':\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs * loader_length, eta_min=0.000005)\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0.000001)\n",
    "        update_per_iteration = False\n",
    "    elif scheduler == 'warmcos':\n",
    "        # warm_cosine = lambda i: min((i + 1) / 3, (1 + math.cos(math.pi * i / (epochs * loader_length))) / 2)\n",
    "        warm_cosine = lambda i: min((i + 1) / 3, (1 + math.cos(math.pi * i / epochs)) / 2)\n",
    "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=warm_cosine)\n",
    "        update_per_iteration = False\n",
    "    elif scheduler == 'multistep':\n",
    "        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=scheduler_tau, gamma=scheduler_gamma)\n",
    "        update_per_iteration = False\n",
    "    elif scheduler == 'warmstep':\n",
    "        warm_step = lambda i: min((i + 1) / 100, 1) * 0.1 ** (i // (lr_step * loader_length))\n",
    "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=warm_step)\n",
    "        update_per_iteration = True\n",
    "    else:\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, epochs * loader_length)\n",
    "        update_per_iteration = True\n",
    " \n",
    "    return optimizer, (scheduler, update_per_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(loss):\n",
    "    if loss == 'bce':\n",
    "        return BinaryCrossEntropyWithLogits()\n",
    "    else:\n",
    "        raise Exception('Unsupported loss {}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletSampler():\n",
    "    def __init__(self, labels, batch_size, nn_inds_path, num_candidates, gnd_data, min_pos=1):\n",
    "        self.batch_size     = batch_size\n",
    "        self.num_candidates = num_candidates\n",
    "        self.cache_nn_inds  = pickle_load(nn_inds_path)\n",
    "        self.labels = labels\n",
    "        self.gnd_data = gnd_data\n",
    "        print('nn_inds_path: ', nn_inds_path)\n",
    "        print('labels len: ', len(labels))\n",
    "        assert (len(self.cache_nn_inds) == len(labels))\n",
    "        #############################################################################\n",
    "        ## Collect valid tuples\n",
    "        valids = np.zeros_like(labels)\n",
    "        for i in range(len(self.cache_nn_inds)):\n",
    "            positives = self.gnd_data[i]['r_easy']\n",
    "            negatives = self.gnd_data[i]['r_junk']\n",
    "            if len(positives) < min_pos or len(negatives) < min_pos:\n",
    "                continue\n",
    "            valids[i] = 1\n",
    "        self.valids = np.where(valids > 0)[0]\n",
    "        self.num_samples = len(self.valids)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        cands = torch.randperm(self.num_samples).tolist()\n",
    "        for i in range(len(cands)):\n",
    "            query_idx = self.valids[cands[i]]\n",
    "            anchor_idx = self.gnd_data[query_idx]['anchor_idx']\n",
    "            \n",
    "            positive_inds = self.gnd_data[query_idx]['g_easy']\n",
    "            negative_inds = self.gnd_data[query_idx]['g_junk']\n",
    "            assert(len(positive_inds) > 0)\n",
    "            assert(len(negative_inds) > 0)\n",
    "\n",
    "            random.shuffle(positive_inds)\n",
    "            random.shuffle(negative_inds)\n",
    "\n",
    "            batch.append(anchor_idx)\n",
    "            batch.append(positive_inds[0]) \n",
    "            batch.append(negative_inds[0])\n",
    "\n",
    "            if len(batch) >= self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                \n",
    "        if len(batch) > 0:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.num_samples * 3 + self.batch_size - 1) // self.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_global_features, num_local_features, seq_len, dim_K, dim_feedforward, nhead, num_encoder_layers, dropout, activation, normalize_before, use_pos):\n",
    "    \n",
    "    if use_pos:\n",
    "        return PosMatchERT(d_global=num_global_features, d_model=num_local_features, seq_len=seq_len, d_K=dim_K, nhead=nhead, num_encoder_layers=num_encoder_layers, \n",
    "            dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, normalize_before=normalize_before)\n",
    "    \n",
    "    else:\n",
    "        return MatchERT(d_global=num_global_features, d_model=num_local_features, seq_len=seq_len, d_K=dim_K, nhead=nhead, num_encoder_layers=num_encoder_layers, \n",
    "            dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, normalize_before=normalize_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() and not cpu else 'cpu')\n",
    "# callback = VisdomLogger(port=visdom_port) if visdom_port else None\n",
    "if cudnn_flag == 'deterministic':\n",
    "    setattr(cudnn, cudnn_flag, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"name = 'tuto_viquae_tuto_r50_gldv2'\\nset_name = 'tuto'\\ntrain_txt = ('tuto_query.txt', 'tuto_gallery.txt')\\ntest_txt = ('tuto_query.txt', 'tuto_selection.txt')\\ntrain_data_dir = 'data/viquae_for_rrt'\\ntest_data_dir  = 'data/viquae_for_rrt'\\ntest_gnd_file = 'gnd_tuto.pkl'\\ntrain_gnd_file = 'gnd_'+set_name+'.pkl'\\n#train_gnd_file = 'gnd_'+set_name+'.pkl'\\ndesc_name = 'r50_gldv2'\\nsampler = 'triplet'\\nsplit_char  = ';;'\\nprefixed = 'non_humans'\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"name = 'tuto_viquae_tuto_r50_gldv2'\n",
    "set_name = 'tuto'\n",
    "train_txt = ('tuto_query.txt', 'tuto_gallery.txt')\n",
    "test_txt = ('tuto_query.txt', 'tuto_selection.txt')\n",
    "train_data_dir = 'data/viquae_for_rrt'\n",
    "test_data_dir  = 'data/viquae_for_rrt'\n",
    "test_gnd_file = 'gnd_tuto.pkl'\n",
    "train_gnd_file = 'gnd_'+set_name+'.pkl'\n",
    "#train_gnd_file = 'gnd_'+set_name+'.pkl'\n",
    "desc_name = 'r50_gldv2'\n",
    "sampler = 'triplet'\n",
    "split_char  = ';;'\n",
    "prefixed = 'non_humans'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'train_viquae_dev_r50_gldv2'\n",
    "set_name = 'train'\n",
    "eval_set_name = 'dev'\n",
    "train_txt = ('train_query.txt', 'train_gallery.txt')\n",
    "test_txt = ('dev_query.txt', 'dev_selection.txt')\n",
    "train_data_dir = 'data/viquae_for_rrt'\n",
    "test_data_dir  = 'data/viquae_for_rrt'\n",
    "train_gnd_file = 'gnd_train.pkl'\n",
    "test_gnd_file = 'gnd_dev.pkl'\n",
    "desc_name = 'r50_gldv2'\n",
    "sampler = 'triplet'\n",
    "split_char  = ';;'\n",
    "prefixed = 'non_humans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size      = 36\n",
    "test_batch_size = 36\n",
    "max_sequence_len = 500\n",
    "num_workers = 8  # number of workers used ot load the data\n",
    "pin_memory  = True  # use the pin_memory option of DataLoader \n",
    "num_candidates = 100\n",
    "recalls = [1, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'rrt'\n",
    "num_global_features = 2048  \n",
    "num_local_features = 128  \n",
    "seq_len = 1004\n",
    "dim_K = 256\n",
    "dim_feedforward = 1024\n",
    "nhead = 4\n",
    "num_encoder_layers = 6\n",
    "dropout = 0.2\n",
    "activation = \"relu\"\n",
    "normalize_before = False\n",
    "use_pos = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '/mnt/beegfs/home/smessoud/RerankingTransformer/models/research/delf/delf/python/delg/data/viquae_for_rrt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "loaders, recall_ks = get_loaders(desc_name=desc_name,\n",
    "    train_data_dir=train_data_dir, \n",
    "    batch_size=36, test_batch_size=36, \n",
    "    num_workers=8, pin_memory=True, \n",
    "    sampler='random', recalls=[1, 5, 10], \n",
    "    set_name=set_name, eval_set_name=None,\n",
    "    train_gnd_file=None, prefixed=prefixed, num_candidates=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders, recall_ks = get_loaders_limit_1(desc_name=desc_name,\n",
    "    train_data_dir=train_data_dir, \n",
    "    batch_size=36, test_batch_size=36, \n",
    "    num_workers=8, pin_memory=True, \n",
    "    sampler='random', recalls=[1, 5, 10], \n",
    "    set_name=set_name, eval_set_name=None,\n",
    "    train_gnd_file=None, prefixed=prefixed, num_candidates=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_inds_path = loaders.set_name + '_nn_inds_%s.pkl'%loaders.query.dataset.desc_name\n",
    "nn_inds_path = nn_inds_path if loaders.prefixed is None else loaders.prefixed+'_'+nn_inds_path\n",
    "nn_inds_path = osp.join(loaders.query.dataset.data_dir, nn_inds_path)\n",
    "cache_nn_inds = torch.from_numpy(pickle_load(nn_inds_path)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Model\n",
    "torch.manual_seed(seed+1)\n",
    "\n",
    "model = get_model(num_global_features,\n",
    "                  num_local_features,\n",
    "                  seq_len,dim_K,\n",
    "                  dim_feedforward,\n",
    "                  nhead,\n",
    "                  num_encoder_layers,\n",
    "                  dropout,\n",
    "                  activation,\n",
    "                  normalize_before,\n",
    "                  use_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freeze Parametes\n",
    "if classifier:\n",
    "    #freeze all layers of the model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    #unfreeze the classfication layers\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "if transformer:\n",
    "    #freeze all layers of the model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.seg_encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.scale_encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.remap.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if last_layers:\n",
    "    #freeze all layers of the model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.seg_encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.scale_encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.remap.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of trainable parameters:  2243201\n"
     ]
    }
   ],
   "source": [
    "if resume is not None:\n",
    "    checkpoint = torch.load(resume, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['state'], strict=True)\n",
    "print('# of trainable parameters: ', num_of_trainable_params(model))\n",
    "class_loss = get_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed+2)\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)\n",
    "parameters = []\n",
    "if no_bias_decay:\n",
    "    parameters.append({'params': [par for par in model.parameters() if par.dim() != 1]})\n",
    "    parameters.append({'params': [par for par in model.parameters() if par.dim() == 1], 'weight_decay': 0})\n",
    "else:\n",
    "    parameters.append({'params': model.parameters()})\n",
    "optimizer, scheduler = get_optimizer_scheduler(parameters=parameters, loader_length=len(loaders.train),\n",
    "                                               optim=optim, epochs=epochs, lr=lr,\n",
    "                                               momentum=momentum, nesterov=nesterov, weight_decay=weight_decay,\n",
    "                                               scheduler=scheduler, scheduler_tau=scheduler_tau, \n",
    "                                               scheduler_gamma=scheduler_gamma, lr_step=None)\n",
    "if resume is not None and checkpoint.get('optim', None) is not None:\n",
    "    optimizer.load_state_dict(checkpoint['optim'])\n",
    "    del checkpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_entry_once(\n",
    "    query_loader: DataLoader,\n",
    "    gallery_loader: DataLoader):\n",
    "    \n",
    "    query_global, query_local, query_mask, query_scales, query_positions, query_names = [], [], [], [], [], []\n",
    "    gallery_global, gallery_local, gallery_mask, gallery_scales, gallery_positions, gallery_names = [], [], [], [], [], []\n",
    "\n",
    "    print(\"READING ENTRIES FOR THE FIRST TIME\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for entry in tqdm(query_loader, desc='Extracting query features', leave=False, ncols=80):\n",
    "            q_global, q_local, q_mask, q_scales, q_positions, _, q_names = entry\n",
    "            query_global.append(q_global.cpu())\n",
    "            query_local.append(q_local.cpu())\n",
    "            query_mask.append(q_mask.cpu())\n",
    "            query_scales.append(q_scales.cpu())\n",
    "            query_positions.append(q_positions.cpu())\n",
    "            query_names.extend(list(q_names))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        query_global    = torch.cat(query_global, 0)\n",
    "        query_local     = torch.cat(query_local, 0)\n",
    "        query_mask      = torch.cat(query_mask, 0)\n",
    "        query_scales    = torch.cat(query_scales, 0)\n",
    "        query_positions = torch.cat(query_positions, 0)\n",
    "\n",
    "        for entry in tqdm(gallery_loader, desc='Extracting gallery features', leave=False, ncols=80):\n",
    "            g_global, g_local, g_mask, g_scales, g_positions, _, g_names = entry\n",
    "            gallery_global.append(g_global.cpu())\n",
    "            gallery_local.append(g_local.cpu())\n",
    "            gallery_mask.append(g_mask.cpu())\n",
    "            gallery_scales.append(g_scales.cpu())\n",
    "            gallery_positions.append(g_positions.cpu())\n",
    "            gallery_names.extend(list(g_names))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        gallery_global    = torch.cat(gallery_global, 0)\n",
    "        gallery_local     = torch.cat(gallery_local, 0)\n",
    "        gallery_mask      = torch.cat(gallery_mask, 0)\n",
    "        gallery_scales    = torch.cat(gallery_scales, 0)\n",
    "        gallery_positions = torch.cat(gallery_positions, 0)\n",
    "    \n",
    "    query_feats   = [query_global, query_local, query_mask, query_scales, query_positions, query_names]\n",
    "    gallery_feats = [gallery_global, gallery_local, gallery_mask, gallery_scales, gallery_positions, gallery_names]\n",
    "    \n",
    "    return query_feats, gallery_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import mean_average_precision_viquae_rerank\n",
    "\n",
    "def fast_evaluate_viquae(\n",
    "    model: nn.Module,\n",
    "    cache_nn_inds: torch.Tensor,\n",
    "    query_loader: DataLoader,\n",
    "    gallery_loader: DataLoader,\n",
    "    recall: List[int],\n",
    "    query_feats, \n",
    "    gallery_feats):\n",
    "    \n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    to_device = lambda x: x.to(device, non_blocking=True)\n",
    "    \n",
    "    if len(query_feats) == 0:\n",
    "        query_feats, gallery_feats = read_entry_once(query_loader, gallery_loader)\n",
    "    \n",
    "    query_global, query_local, query_mask, query_scales, query_positions, query_names = query_feats\n",
    "    gallery_global, gallery_local, gallery_mask, gallery_scales, gallery_positions, gallery_names = gallery_feats\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    fast_evaluate_function = partial(mean_average_precision_viquae_rerank, model=model, cache_nn_inds=cache_nn_inds,\n",
    "        query_global=query_global, query_local=query_local, query_mask=query_mask, query_scales=query_scales, query_positions=query_positions, \n",
    "        gallery_global=gallery_global, gallery_local=gallery_local, gallery_mask=gallery_mask, gallery_scales=gallery_scales, gallery_positions=gallery_positions, \n",
    "        ks=recall, \n",
    "        gnd=query_loader.dataset.gnd_data,\n",
    "    )\n",
    "    metrics = fast_evaluate_function()\n",
    "    \n",
    "    return metrics, query_feats, gallery_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnd = pickle_load(osp.join(train_data_dir, train_gnd_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image = gnd['qimlist'][10]\n",
    "postive_image = gnd['gnd'][10]['r_easy'][0]\n",
    "negative_image = gnd['gnd'][10]['r_junk'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_features, l_features, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_feats, gallery_feats = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-dc58d0f51897>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_nn_inds_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_set_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_nn_inds_%s.pkl'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mloaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meval_nn_inds_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_nn_inds_path\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefixed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefixed\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0meval_nn_inds_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0meval_nn_inds_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_nn_inds_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nn_inds_path:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_nn_inds_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcache_nn_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_nn_inds_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'str'"
     ]
    }
   ],
   "source": [
    "eval_nn_inds_path = loaders.eval_set_name + '_nn_inds_%s.pkl'%loaders.query.dataset.desc_name\n",
    "eval_nn_inds_path = eval_nn_inds_path if loaders.prefixed is None else loaders.prefixed+'_'+eval_nn_inds_path\n",
    "eval_nn_inds_path = osp.join(loaders.query.dataset.data_dir, eval_nn_inds_path)\n",
    "print('nn_inds_path:', eval_nn_inds_path)\n",
    "cache_nn_inds = torch.from_numpy(pickle_load(eval_nn_inds_path)).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed+3)\n",
    "# setup partial function to simplify call\n",
    "\n",
    "eval_function = partial(fast_evaluate_viquae, model=model, \n",
    "    cache_nn_inds=cache_nn_inds,\n",
    "    recall=recall_ks, query_loader=loaders.query, gallery_loader=loaders.gallery,\n",
    "    query_feats=query_feats, gallery_feats=gallery_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Extracting query features:   0%|                         | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING ENTRIES FOR THE FIRST TIME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 656/656 [06:42<00:00,  1.63it/s]                               \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-24b77dbf7a0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgallery_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbest_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-b357d2a0913c>\u001b[0m in \u001b[0;36mfast_evaluate_viquae\u001b[0;34m(model, cache_nn_inds, query_loader, gallery_loader, recall, query_feats, gallery_feats)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mgnd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgnd_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     )\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_evaluate_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgallery_feats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rrt/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RerankingTransformer/RRT_GLD/utils/metrics.py\u001b[0m in \u001b[0;36mmean_average_precision_viquae_rerank\u001b[0;34m(model, cache_nn_inds, query_global, query_local, query_mask, query_scales, query_positions, gallery_global, gallery_local, gallery_mask, gallery_scales, gallery_positions, ks, gnd, set_name, max_sequence_len)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mranks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosest_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_padded_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_nn_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'viquae'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgnd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gnd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sequence_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RerankingTransformer/RRT_GLD/utils/metrics.py\u001b[0m in \u001b[0;36mremove_padded_indices\u001b[0;34m(rankings, nn_inds, sizes)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_padded_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrankings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# rankings.shape -> (nb_queries x 100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrankings\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_inds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result, query_feats, gallery_feats = eval_function()\n",
    "pprint(result)\n",
    "best_val = (0, result, deepcopy(model.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8654e8b7b0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed+4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = osp.join(temp_dir, 'rrt_tuto_viquae_dev_r50_gldv2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'map': 29.87,\n",
       " 'mrr': 39.06,\n",
       " 'precision': 11.82,\n",
       " 'hit_rate': 85.96,\n",
       " 'recall': 85.09,\n",
       " 'map@1': 11.9,\n",
       " 'mrr@1': 28.07,\n",
       " 'precision@1': 28.07,\n",
       " 'hit_rate@1': 28.07,\n",
       " 'recall@1': 11.9,\n",
       " 'map@5': 17.57,\n",
       " 'mrr@5': 36.08,\n",
       " 'precision@5': 17.54,\n",
       " 'hit_rate@5': 49.12,\n",
       " 'recall@5': 21.97,\n",
       " 'map@10': 19.82,\n",
       " 'mrr@10': 37.97,\n",
       " 'precision@10': 14.56,\n",
       " 'hit_rate@10': 61.4,\n",
       " 'recall@10': 28.76}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "        model: nn.Module,\n",
    "        loader: DataLoader,\n",
    "        class_loss: nn.Module,\n",
    "        optimizer: Optimizer,\n",
    "        # the second entry indicates if the scheduler should step per iteration or epoch\n",
    "        scheduler: (_LRScheduler, bool), \n",
    "        max_norm: float,\n",
    "        epoch: int,\n",
    "        writer: SummaryWriter,\n",
    "        # callback: VisdomLogger,\n",
    "        freq: int,\n",
    "        ex: Experiment = None) -> None:\n",
    "    model.train()\n",
    "    device = next(model.parameters()).device\n",
    "    to_device = lambda x: x.to(device, non_blocking=True)\n",
    "    loader_length = len(loader)\n",
    "    train_losses = AverageMeter(device=device, length=loader_length)\n",
    "    train_accs = AverageMeter(device=device, length=loader_length)\n",
    "    pbar = tqdm(loader, ncols=80, desc='Training   [{:03d}]'.format(epoch))\n",
    "    for i, entry in enumerate(pbar):\n",
    "        global_feats, local_feats, local_mask, scales, positions, _, _ = entry\n",
    "        global_feats, local_feats, local_mask, scales, positions = map(to_device, (global_feats, local_feats, local_mask, scales, positions))\n",
    "        \n",
    "        p_logits = model(global_feats[0::3], local_feats[0::3], local_mask[0::3], scales[0::3], positions[0::3],\n",
    "            global_feats[1::3], local_feats[1::3], local_mask[1::3], scales[1::3], positions[1::3])\n",
    "        n_logits = model(global_feats[0::3], local_feats[0::3], local_mask[0::3], scales[0::3], positions[0::3],\n",
    "            global_feats[2::3], local_feats[2::3], local_mask[2::3], scales[2::3], positions[2::3])\n",
    "\n",
    "        logits = torch.cat([p_logits, n_logits], 0)\n",
    "        bsize = logits.size(0)\n",
    "        # assert (bsize % 2 == 0)\n",
    "        labels = logits.new_ones(logits.size()).float()\n",
    "        labels[(bsize//2):] = 0\n",
    "        loss = class_loss(logits, labels).mean()\n",
    "        acc = ((torch.sigmoid(logits) > 0.5).long() == labels.long()).float().mean()\n",
    "\n",
    "        ##############################################\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if max_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler[-1]:\n",
    "            scheduler[0].step()\n",
    "\n",
    "        train_losses.append(loss)\n",
    "        train_accs.append(acc)\n",
    "\n",
    "\n",
    "        step = epoch + i / loader_length\n",
    "        print('step/loss/accu/lr:', step, train_losses.last_avg.item(), train_accs.last_avg.item(), scheduler[0].get_last_lr()[0])\n",
    "        writer.add_scalar('loss/step', train_losses.last_avg.item(),  step)\n",
    "        writer.add_scalar('accu/step', train_accs.last_avg.item(),    step)\n",
    "        writer.add_scalar('lr/step',   scheduler[0].get_last_lr()[0], step)\n",
    "\n",
    "    if not scheduler[-1]:\n",
    "        scheduler[0].step()\n",
    "\n",
    "    if ex is not None:\n",
    "        for i, (loss, acc) in enumerate(zip(train_losses.values_list, train_accs.values_list)):\n",
    "            step = epoch + i / loader_length\n",
    "            ex.log_scalar('train.loss', loss, step=step)\n",
    "            ex.log_scalar('train.acc',  acc, step=step)\n",
    "    \n",
    "    print('STATISTICS FOR EPOCH ', epoch)\n",
    "    step = epoch + i / loader_length\n",
    "    print('step/loss/accu/lr:', step, train_losses.last_avg.item(), train_accs.last_avg.item(), scheduler[0].get_last_lr()[0])\n",
    "    writer.add_scalar('loss/epoch', train_losses.last_avg.item(),  epoch)\n",
    "    writer.add_scalar('accu/epoch', train_accs.last_avg.item(),    epoch)\n",
    "    writer.add_scalar('lr/epoch',   scheduler[0].get_last_lr()[0], epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [000]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step/loss/accu/lr: 0.0 1.516977071762085 0.5 0.0001\n",
      "step/loss/accu/lr: 0.5 1.8169175386428833 0.4285714626312256 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [000]: 100%|███████████████████████████| 2/2 [00:02<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICS FOR EPOCH  0\n",
      "epoch/loss/accu/lr: 0 1.8169175386428833 0.4285714626312256 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:33<00:00,  1.72it/s]\n",
      "Training   [001]:   0%|                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': 29.87, 'mrr': 39.05, 'precision': 11.82, 'hit_rate': 85.96, 'recall': 85.09, 'map@1': 11.9, 'mrr@1': 28.07, 'precision@1': 28.07, 'hit_rate@1': 28.07, 'recall@1': 11.9, 'map@5': 17.57, 'mrr@5': 36.08, 'precision@5': 17.54, 'hit_rate@5': 49.12, 'recall@5': 21.97, 'map@10': 19.86, 'mrr@10': 37.97, 'precision@10': 14.74, 'hit_rate@10': 61.4, 'recall@10': 28.85}\n",
      "Validation [000]\n",
      "{'hit_rate': 85.96,\n",
      " 'hit_rate@1': 28.07,\n",
      " 'hit_rate@10': 61.4,\n",
      " 'hit_rate@5': 49.12,\n",
      " 'map': 29.87,\n",
      " 'map@1': 11.9,\n",
      " 'map@10': 19.86,\n",
      " 'map@5': 17.57,\n",
      " 'mrr': 39.05,\n",
      " 'mrr@1': 28.07,\n",
      " 'mrr@10': 37.97,\n",
      " 'mrr@5': 36.08,\n",
      " 'precision': 11.82,\n",
      " 'precision@1': 28.07,\n",
      " 'precision@10': 14.74,\n",
      " 'precision@5': 17.54,\n",
      " 'recall': 85.09,\n",
      " 'recall@1': 11.9,\n",
      " 'recall@10': 28.85,\n",
      " 'recall@5': 21.97}\n",
      "New best model in epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [001]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step/loss/accu/lr: 1.0 1.3412506580352783 0.5833333730697632 1e-05\n",
      "step/loss/accu/lr: 1.5 1.7440519332885742 0.5 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [001]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICS FOR EPOCH  1\n",
      "epoch/loss/accu/lr: 1 1.7440519332885742 0.5 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:33<00:00,  1.72it/s]\n",
      "Training   [002]:   0%|                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': 29.82, 'mrr': 39.23, 'precision': 11.82, 'hit_rate': 85.96, 'recall': 85.09, 'map@1': 11.9, 'mrr@1': 28.07, 'precision@1': 28.07, 'hit_rate@1': 28.07, 'recall@1': 11.9, 'map@5': 17.55, 'mrr@5': 36.29, 'precision@5': 17.54, 'hit_rate@5': 49.12, 'recall@5': 21.97, 'map@10': 19.78, 'mrr@10': 38.15, 'precision@10': 14.56, 'hit_rate@10': 61.4, 'recall@10': 28.78}\n",
      "Validation [001]\n",
      "{'hit_rate': 85.96,\n",
      " 'hit_rate@1': 28.07,\n",
      " 'hit_rate@10': 61.4,\n",
      " 'hit_rate@5': 49.12,\n",
      " 'map': 29.82,\n",
      " 'map@1': 11.9,\n",
      " 'map@10': 19.78,\n",
      " 'map@5': 17.55,\n",
      " 'mrr': 39.23,\n",
      " 'mrr@1': 28.07,\n",
      " 'mrr@10': 38.15,\n",
      " 'mrr@5': 36.29,\n",
      " 'precision': 11.82,\n",
      " 'precision@1': 28.07,\n",
      " 'precision@10': 14.56,\n",
      " 'precision@5': 17.54,\n",
      " 'recall': 85.09,\n",
      " 'recall@1': 11.9,\n",
      " 'recall@10': 28.78,\n",
      " 'recall@5': 21.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [002]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step/loss/accu/lr: 2.0 1.97579026222229 0.4166666865348816 1e-05\n",
      "step/loss/accu/lr: 2.5 1.8234694004058838 0.4285714626312256 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [002]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICS FOR EPOCH  2\n",
      "epoch/loss/accu/lr: 2 1.8234694004058838 0.4285714626312256 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:33<00:00,  1.72it/s]\n",
      "Training   [003]:   0%|                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': 29.63, 'mrr': 39.19, 'precision': 11.82, 'hit_rate': 85.96, 'recall': 85.09, 'map@1': 11.9, 'mrr@1': 28.07, 'precision@1': 28.07, 'hit_rate@1': 28.07, 'recall@1': 11.9, 'map@5': 17.25, 'mrr@5': 35.94, 'precision@5': 16.84, 'hit_rate@5': 47.37, 'recall@5': 21.6, 'map@10': 19.58, 'mrr@10': 38.09, 'precision@10': 14.56, 'hit_rate@10': 61.4, 'recall@10': 28.78}\n",
      "Validation [002]\n",
      "{'hit_rate': 85.96,\n",
      " 'hit_rate@1': 28.07,\n",
      " 'hit_rate@10': 61.4,\n",
      " 'hit_rate@5': 47.37,\n",
      " 'map': 29.63,\n",
      " 'map@1': 11.9,\n",
      " 'map@10': 19.58,\n",
      " 'map@5': 17.25,\n",
      " 'mrr': 39.19,\n",
      " 'mrr@1': 28.07,\n",
      " 'mrr@10': 38.09,\n",
      " 'mrr@5': 35.94,\n",
      " 'precision': 11.82,\n",
      " 'precision@1': 28.07,\n",
      " 'precision@10': 14.56,\n",
      " 'precision@5': 16.84,\n",
      " 'recall': 85.09,\n",
      " 'recall@1': 11.9,\n",
      " 'recall@10': 28.78,\n",
      " 'recall@5': 21.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [003]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step/loss/accu/lr: 3.0 0.9400882720947266 0.5833333730697632 1e-05\n",
      "step/loss/accu/lr: 3.5 0.9248397350311279 0.5714285969734192 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [003]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICS FOR EPOCH  3\n",
      "epoch/loss/accu/lr: 3 0.9248397350311279 0.5714285969734192 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:33<00:00,  1.72it/s]\n",
      "Training   [004]:   0%|                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': 29.67, 'mrr': 40.09, 'precision': 11.82, 'hit_rate': 85.96, 'recall': 85.09, 'map@1': 11.99, 'mrr@1': 29.82, 'precision@1': 29.82, 'hit_rate@1': 29.82, 'recall@1': 11.99, 'map@5': 17.0, 'mrr@5': 36.81, 'precision@5': 16.84, 'hit_rate@5': 47.37, 'recall@5': 20.07, 'map@10': 19.68, 'mrr@10': 39.0, 'precision@10': 14.74, 'hit_rate@10': 61.4, 'recall@10': 29.08}\n",
      "Validation [003]\n",
      "{'hit_rate': 85.96,\n",
      " 'hit_rate@1': 29.82,\n",
      " 'hit_rate@10': 61.4,\n",
      " 'hit_rate@5': 47.37,\n",
      " 'map': 29.67,\n",
      " 'map@1': 11.99,\n",
      " 'map@10': 19.68,\n",
      " 'map@5': 17.0,\n",
      " 'mrr': 40.09,\n",
      " 'mrr@1': 29.82,\n",
      " 'mrr@10': 39.0,\n",
      " 'mrr@5': 36.81,\n",
      " 'precision': 11.82,\n",
      " 'precision@1': 29.82,\n",
      " 'precision@10': 14.74,\n",
      " 'precision@5': 16.84,\n",
      " 'recall': 85.09,\n",
      " 'recall@1': 11.99,\n",
      " 'recall@10': 29.08,\n",
      " 'recall@5': 20.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [004]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step/loss/accu/lr: 4.0 1.4032745361328125 0.4166666865348816 1e-05\n",
      "step/loss/accu/lr: 4.5 1.608733057975769 0.2857142984867096 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [004]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICS FOR EPOCH  4\n",
      "epoch/loss/accu/lr: 4 1.608733057975769 0.2857142984867096 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:33<00:00,  1.72it/s]\n",
      "Training   [005]:   0%|                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': 29.65, 'mrr': 40.06, 'precision': 11.82, 'hit_rate': 85.96, 'recall': 85.09, 'map@1': 11.99, 'mrr@1': 29.82, 'precision@1': 29.82, 'hit_rate@1': 29.82, 'recall@1': 11.99, 'map@5': 17.0, 'mrr@5': 36.81, 'precision@5': 16.84, 'hit_rate@5': 47.37, 'recall@5': 20.07, 'map@10': 19.59, 'mrr@10': 38.95, 'precision@10': 14.56, 'hit_rate@10': 61.4, 'recall@10': 28.98}\n",
      "Validation [004]\n",
      "{'hit_rate': 85.96,\n",
      " 'hit_rate@1': 29.82,\n",
      " 'hit_rate@10': 61.4,\n",
      " 'hit_rate@5': 47.37,\n",
      " 'map': 29.65,\n",
      " 'map@1': 11.99,\n",
      " 'map@10': 19.59,\n",
      " 'map@5': 17.0,\n",
      " 'mrr': 40.06,\n",
      " 'mrr@1': 29.82,\n",
      " 'mrr@10': 38.95,\n",
      " 'mrr@5': 36.81,\n",
      " 'precision': 11.82,\n",
      " 'precision@1': 29.82,\n",
      " 'precision@10': 14.56,\n",
      " 'precision@5': 16.84,\n",
      " 'recall': 85.09,\n",
      " 'recall@1': 11.99,\n",
      " 'recall@10': 28.98,\n",
      " 'recall@5': 20.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [005]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step/loss/accu/lr: 5.0 1.389216661453247 0.4583333432674408 1e-05\n",
      "step/loss/accu/lr: 5.5 1.0090025663375854 0.5714285969734192 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [005]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICS FOR EPOCH  5\n",
      "epoch/loss/accu/lr: 5 1.0090025663375854 0.5714285969734192 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:33<00:00,  1.72it/s]\n",
      "Training   [006]:   0%|                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': 29.73, 'mrr': 41.04, 'precision': 11.82, 'hit_rate': 85.96, 'recall': 85.09, 'map@1': 12.18, 'mrr@1': 31.58, 'precision@1': 31.58, 'hit_rate@1': 31.58, 'recall@1': 12.18, 'map@5': 17.11, 'mrr@5': 37.84, 'precision@5': 16.84, 'hit_rate@5': 47.37, 'recall@5': 20.07, 'map@10': 19.67, 'mrr@10': 39.94, 'precision@10': 14.56, 'hit_rate@10': 61.4, 'recall@10': 28.98}\n",
      "Validation [005]\n",
      "{'hit_rate': 85.96,\n",
      " 'hit_rate@1': 31.58,\n",
      " 'hit_rate@10': 61.4,\n",
      " 'hit_rate@5': 47.37,\n",
      " 'map': 29.73,\n",
      " 'map@1': 12.18,\n",
      " 'map@10': 19.67,\n",
      " 'map@5': 17.11,\n",
      " 'mrr': 41.04,\n",
      " 'mrr@1': 31.58,\n",
      " 'mrr@10': 39.94,\n",
      " 'mrr@5': 37.84,\n",
      " 'precision': 11.82,\n",
      " 'precision@1': 31.58,\n",
      " 'precision@10': 14.56,\n",
      " 'precision@5': 16.84,\n",
      " 'recall': 85.09,\n",
      " 'recall@1': 12.18,\n",
      " 'recall@10': 28.98,\n",
      " 'recall@5': 20.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [006]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step/loss/accu/lr: 6.0 1.1189051866531372 0.5416666865348816 1e-05\n",
      "step/loss/accu/lr: 6.5 0.9911366105079651 0.5 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [006]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICS FOR EPOCH  6\n",
      "epoch/loss/accu/lr: 6 0.9911366105079651 0.5 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:33<00:00,  1.72it/s]\n",
      "Training   [007]:   0%|                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': 29.68, 'mrr': 40.96, 'precision': 11.82, 'hit_rate': 85.96, 'recall': 85.09, 'map@1': 12.18, 'mrr@1': 31.58, 'precision@1': 31.58, 'hit_rate@1': 31.58, 'recall@1': 12.18, 'map@5': 17.04, 'mrr@5': 37.49, 'precision@5': 16.49, 'hit_rate@5': 45.61, 'recall@5': 19.72, 'map@10': 19.7, 'mrr@10': 39.86, 'precision@10': 14.74, 'hit_rate@10': 61.4, 'recall@10': 29.1}\n",
      "Validation [006]\n",
      "{'hit_rate': 85.96,\n",
      " 'hit_rate@1': 31.58,\n",
      " 'hit_rate@10': 61.4,\n",
      " 'hit_rate@5': 45.61,\n",
      " 'map': 29.68,\n",
      " 'map@1': 12.18,\n",
      " 'map@10': 19.7,\n",
      " 'map@5': 17.04,\n",
      " 'mrr': 40.96,\n",
      " 'mrr@1': 31.58,\n",
      " 'mrr@10': 39.86,\n",
      " 'mrr@5': 37.49,\n",
      " 'precision': 11.82,\n",
      " 'precision@1': 31.58,\n",
      " 'precision@10': 14.74,\n",
      " 'precision@5': 16.49,\n",
      " 'recall': 85.09,\n",
      " 'recall@1': 12.18,\n",
      " 'recall@10': 29.1,\n",
      " 'recall@5': 19.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [007]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step/loss/accu/lr: 7.0 0.9094319343566895 0.6666666865348816 1e-05\n",
      "step/loss/accu/lr: 7.5 1.585361361503601 0.5 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [007]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICS FOR EPOCH  7\n",
      "epoch/loss/accu/lr: 7 1.585361361503601 0.5 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:33<00:00,  1.72it/s]\n",
      "Training   [008]:   0%|                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': 29.74, 'mrr': 41.01, 'precision': 11.82, 'hit_rate': 85.96, 'recall': 85.09, 'map@1': 12.18, 'mrr@1': 31.58, 'precision@1': 31.58, 'hit_rate@1': 31.58, 'recall@1': 12.18, 'map@5': 17.05, 'mrr@5': 37.57, 'precision@5': 16.49, 'hit_rate@5': 45.61, 'recall@5': 19.72, 'map@10': 19.72, 'mrr@10': 39.9, 'precision@10': 14.74, 'hit_rate@10': 61.4, 'recall@10': 29.16}\n",
      "Validation [007]\n",
      "{'hit_rate': 85.96,\n",
      " 'hit_rate@1': 31.58,\n",
      " 'hit_rate@10': 61.4,\n",
      " 'hit_rate@5': 45.61,\n",
      " 'map': 29.74,\n",
      " 'map@1': 12.18,\n",
      " 'map@10': 19.72,\n",
      " 'map@5': 17.05,\n",
      " 'mrr': 41.01,\n",
      " 'mrr@1': 31.58,\n",
      " 'mrr@10': 39.9,\n",
      " 'mrr@5': 37.57,\n",
      " 'precision': 11.82,\n",
      " 'precision@1': 31.58,\n",
      " 'precision@10': 14.74,\n",
      " 'precision@5': 16.49,\n",
      " 'recall': 85.09,\n",
      " 'recall@1': 12.18,\n",
      " 'recall@10': 29.16,\n",
      " 'recall@5': 19.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [008]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step/loss/accu/lr: 8.0 1.05882728099823 0.5 1e-05\n",
      "step/loss/accu/lr: 8.5 0.9301155209541321 0.5 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [008]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICS FOR EPOCH  8\n",
      "epoch/loss/accu/lr: 8 0.9301155209541321 0.5 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:33<00:00,  1.72it/s]\n",
      "Training   [009]:   0%|                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': 29.73, 'mrr': 40.91, 'precision': 11.82, 'hit_rate': 85.96, 'recall': 85.09, 'map@1': 12.18, 'mrr@1': 31.58, 'precision@1': 31.58, 'hit_rate@1': 31.58, 'recall@1': 12.18, 'map@5': 17.07, 'mrr@5': 37.57, 'precision@5': 16.49, 'hit_rate@5': 45.61, 'recall@5': 19.72, 'map@10': 19.63, 'mrr@10': 39.8, 'precision@10': 14.56, 'hit_rate@10': 61.4, 'recall@10': 28.87}\n",
      "Validation [008]\n",
      "{'hit_rate': 85.96,\n",
      " 'hit_rate@1': 31.58,\n",
      " 'hit_rate@10': 61.4,\n",
      " 'hit_rate@5': 45.61,\n",
      " 'map': 29.73,\n",
      " 'map@1': 12.18,\n",
      " 'map@10': 19.63,\n",
      " 'map@5': 17.07,\n",
      " 'mrr': 40.91,\n",
      " 'mrr@1': 31.58,\n",
      " 'mrr@10': 39.8,\n",
      " 'mrr@5': 37.57,\n",
      " 'precision': 11.82,\n",
      " 'precision@1': 31.58,\n",
      " 'precision@10': 14.56,\n",
      " 'precision@5': 16.49,\n",
      " 'recall': 85.09,\n",
      " 'recall@1': 12.18,\n",
      " 'recall@10': 28.87,\n",
      " 'recall@5': 19.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [009]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step/loss/accu/lr: 9.0 0.7525361776351929 0.625 1e-05\n",
      "step/loss/accu/lr: 9.5 1.198188066482544 0.5714285969734192 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [009]: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICS FOR EPOCH  9\n",
      "epoch/loss/accu/lr: 9 1.198188066482544 0.5714285969734192 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:33<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': 29.76, 'mrr': 40.92, 'precision': 11.82, 'hit_rate': 85.96, 'recall': 85.09, 'map@1': 12.18, 'mrr@1': 31.58, 'precision@1': 31.58, 'hit_rate@1': 31.58, 'recall@1': 12.18, 'map@5': 17.07, 'mrr@5': 37.57, 'precision@5': 16.49, 'hit_rate@5': 45.61, 'recall@5': 19.72, 'map@10': 19.79, 'mrr@10': 39.98, 'precision@10': 15.09, 'hit_rate@10': 63.16, 'recall@10': 29.5}\n",
      "Validation [009]\n",
      "{'hit_rate': 85.96,\n",
      " 'hit_rate@1': 31.58,\n",
      " 'hit_rate@10': 63.16,\n",
      " 'hit_rate@5': 45.61,\n",
      " 'map': 29.76,\n",
      " 'map@1': 12.18,\n",
      " 'map@10': 19.79,\n",
      " 'map@5': 17.07,\n",
      " 'mrr': 40.92,\n",
      " 'mrr@1': 31.58,\n",
      " 'mrr@10': 39.98,\n",
      " 'mrr@5': 37.57,\n",
      " 'precision': 11.82,\n",
      " 'precision@1': 31.58,\n",
      " 'precision@10': 15.09,\n",
      " 'precision@5': 16.49,\n",
      " 'recall': 85.09,\n",
      " 'recall@1': 12.18,\n",
      " 'recall@10': 29.5,\n",
      " 'recall@5': 19.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    if cudnn_flag == 'benchmark':\n",
    "        setattr(cudnn, cudnn_flag, True)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    train_one_epoch(model=model, loader=loaders.train, class_loss=class_loss, writer=writer, optimizer=optimizer, scheduler=scheduler, max_norm=max_norm, epoch=epoch, freq=visdom_freq, ex=None)\n",
    "    \n",
    "    # validation\n",
    "    if cudnn_flag == 'benchmark':\n",
    "        setattr(cudnn, cudnn_flag, False)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    result, query_feats, gallery_feats = fast_evaluate_viquae(model=model,\n",
    "                                                              cache_nn_inds=cache_nn_inds,\n",
    "                                                              recall=recall_ks,\n",
    "                                                              query_loader=loaders.query, \n",
    "                                                              gallery_loader=loaders.gallery,\n",
    "                                                              query_feats=query_feats, \n",
    "                                                              gallery_feats=gallery_feats)\n",
    "    \n",
    "    print('Validation [{:03d}]'.format(epoch)), pprint(result)\n",
    "    #ex.log_scalar('val.map', result['map'], step=epoch + 1)\n",
    "\n",
    "    if result['map'] >= best_val[1]['map']:\n",
    "        print('New best model in epoch %d.'%epoch)\n",
    "        best_val = (epoch + 1, result, deepcopy(model.state_dict()))\n",
    "        #torch.save({'state': state_dict_to_cpu(best_val[2]), 'optim': optimizer.state_dict()}, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entry' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-08070cbdfe97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'entry' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "device = next(model.parameters()).device\n",
    "to_device = lambda x: x.to(device, non_blocking=True)\n",
    "loader_length = len(loader)\n",
    "train_losses = AverageMeter(device=device, length=loader_length)\n",
    "train_accs = AverageMeter(device=device, length=loader_length)\n",
    "pbar = tqdm(loader, ncols=80, desc='Training   [{:03d}]'.format(epoch))\n",
    "for i, entry in enumerate(pbar):\n",
    "    global_feats, local_feats, local_mask, scales, positions, names, _ = entry\n",
    "    global_feats, local_feats, local_mask, scales, positions = map(to_device, (global_feats, local_feats, local_mask, scales, positions))\n",
    "\n",
    "    p_logits = model(global_feats[0::3], local_feats[0::3], local_mask[0::3], scales[0::3], positions[0::3],\n",
    "        global_feats[1::3], local_feats[1::3], local_mask[1::3], scales[1::3], positions[1::3])\n",
    "    n_logits = model(global_feats[0::3], local_feats[0::3], local_mask[0::3], scales[0::3], positions[0::3],\n",
    "        global_feats[2::3], local_feats[2::3], local_mask[2::3], scales[2::3], positions[2::3])\n",
    "\n",
    "    logits = torch.cat([p_logits, n_logits], 0)\n",
    "    bsize = logits.size(0)\n",
    "    # assert (bsize % 2 == 0)\n",
    "    labels = logits.new_ones(logits.size()).float()\n",
    "    labels[(bsize//2):] = 0\n",
    "    loss = class_loss(logits, labels).mean()\n",
    "    acc = ((torch.sigmoid(logits) > 0.5).long() == labels.long()).float().mean()\n",
    "\n",
    "    ##############################################\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    if max_norm > 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "    optimizer.step()\n",
    "\n",
    "    if scheduler[-1]:\n",
    "        scheduler[0].step()\n",
    "\n",
    "    train_losses.append(loss)\n",
    "    train_accs.append(acc)\n",
    "\n",
    "\n",
    "    if not (i + 1) % freq:\n",
    "        step = epoch + i / loader_length\n",
    "        print('step/loss/accu/lr:', step, train_losses.last_avg.item(), train_accs.last_avg.item(), scheduler[0].get_last_lr()[0])\n",
    "    \n",
    "    \n",
    "\n",
    "if not scheduler[-1]:\n",
    "    scheduler[0].step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Triplet Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnd =  pickle_load(osp.join(train_data_dir, prefixed+'_'+train_gnd_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['easy', 'hard', 'junk', 'neg', 'r_easy', 'r_hard', 'r_junk', 'r_neg', 'g_easy', 'g_hard', 'g_junk', 'g_neg', 'provenance_entity', 'ir_order', 'r_ir_order', 'rank_img_dict', 'img_rank_dict', 'anchor_idx', 'is_human'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnd['gnd'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 1, 35, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "len(gnd['gnd'][i]['junk']), len(gnd['gnd'][i]['easy']), len(gnd['gnd'][i]['g_junk']), len(gnd['gnd'][i]['g_easy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = self.gnd_data[i]['r_easy']\n",
    "negatives = self.gnd_data[i]['r_junk']\n",
    "positive_inds = self.gnd_data[query_idx]['g_easy']\n",
    "negative_inds = self.gnd_data[query_idx]['g_junk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gnd_data  = None if train_gnd_file is None else pickle_load(osp.join(train_data_dir, train_gnd_file))\n",
    "train_lines_txt = train_txt[1] if prefixed is None else prefixed+'_'+train_txt[1]\n",
    "train_lines     = read_file(osp.join(train_data_dir, train_lines_txt))\n",
    "train_q_lines_txt = train_txt[0] if prefixed is None else prefixed+'_'+train_txt[0]\n",
    "train_q_lines   = read_file(osp.join(train_data_dir, train_q_lines_txt))\n",
    "train_samples   = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in train_lines]\n",
    "train_q_samples = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in train_q_lines]\n",
    "train_set       = FeatureDataset(train_data_dir, train_samples,   desc_name, max_sequence_len, gnd_data=train_gnd_data)\n",
    "query_train_set = FeatureDataset(train_data_dir, train_q_samples, desc_name, max_sequence_len, gnd_data=train_gnd_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_g_lines   = read_file(osp.join(train_data_dir, prefixed+'_'+'train_gallery.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5564, 57, 42678)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lines), len(train_q_lines), len(train_g_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn_inds_path:  /mnt/beegfs/home/smessoud/RerankingTransformer/models/research/delf/delf/python/delg/data/viquae_for_rrt/non_humans_tuto_nn_inds_r50_gldv2.pkl\n",
      "labels len:  57\n"
     ]
    }
   ],
   "source": [
    "train_nn_inds = osp.join(train_data_dir, prefixed+'_'+set_name + '_nn_inds_%s.pkl'%desc_name)\n",
    "gnd_data = train_set.gnd_data['gnd']\n",
    "train_sampler = TripletSampler(query_train_set.targets, batch_size, train_nn_inds, num_candidates, gnd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2326, 609, 3627, 4026, 5127, 3215, 838, 3761, 3529, 5509, 1796, 863, 4091, 2071, 428, 1551, 97, 723, 543, 5348, 3246, 4696, 3439, 137, 11, 2465, 2966, 62, 2398, 1083, 2238, 2517, 1207, 4274, 1625, 400]\n",
      "[4609, 2698, 1464, 3850, 1412, 3924, 3329, 1159, 5429, 2238, 5242, 999, 3423, 4198, 187, 592, 395, 5468, 1205, 4234, 1351, 961, 755, 661, 592, 5417, 4176, 438, 1614, 3960, 2295, 4781, 5520, 2884, 402, 4977]\n",
      "[629, 4676, 906, 861, 2691, 4718, 2947, 3456, 2642, 4752, 314, 2274, 4754, 4384, 2106, 3010, 5470, 4222, 690, 1385, 2036, 4023, 4100, 4063, 3016, 1296, 2287, 3423, 3395, 81, 605, 1958, 179, 3918, 2675, 1415]\n",
      "[2236, 2876, 822, 3342, 474, 756, 4696, 5502, 2288, 302, 1638, 3356, 1533, 2078, 1407, 929, 1128, 3816, 1551, 531, 4749, 2765, 2796, 4522, 4992, 450, 247, 4253, 5199, 1305, 5396, 5433, 713, 275, 511, 2062]\n",
      "[690, 3193, 1500, 2493, 1402, 1459, 1720, 4395, 1601, 876, 1444, 445, 3315, 3708, 1867, 4685, 4104, 386]\n"
     ]
    }
   ],
   "source": [
    "for item in train_sampler:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
