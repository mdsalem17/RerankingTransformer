{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path as osp\n",
    "import pickle, json, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math\n",
    "import os.path as osp\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacred\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sacred import SETTINGS\n",
    "from sacred.utils import apply_backspaces_and_linefeeds\n",
    "from torch.backends import cudnn\n",
    "from torch.optim import SGD, Adam, AdamW, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.backends import cudnn\n",
    "# from visdom_logger import VisdomLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, BatchSampler\n",
    "from typing import NamedTuple, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.matcher import MatchERT\n",
    "from models.ingredient import model_ingredient, get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/beegfs/home/smessoud/anaconda3/envs/rrt/lib/python3.7/site-packages/ranx/qrels_run_common.py:7: UserWarning: Sorting disabled. Assumes that you provided sorted doc_ids!\n",
      "  warnings.warn(\"Sorting disabled. Assumes that you provided sorted doc_ids!\")\n"
     ]
    }
   ],
   "source": [
    "from models.ingredient import model_ingredient, get_model\n",
    "from utils import state_dict_to_cpu, num_of_trainable_params\n",
    "from utils import pickle_load, pickle_save\n",
    "#from utils.data.utils import TripletSampler\n",
    "from utils import BinaryCrossEntropyWithLogits\n",
    "from utils.data.dataset_ingredient import data_ingredient, get_loaders\n",
    "from utils.training import train_one_epoch, evaluate_viquae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import pickle_load\n",
    "from sacred import Experiment\n",
    "from utils.data.dataset_ingredient import data_ingredient, get_loaders\n",
    "from utils.data.dataset import FeatureDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = sacred.Experiment('RRT Training', ingredients=[data_ingredient, model_ingredient], interactive=True)\n",
    "# Filter backspaces and linefeeds\n",
    "SETTINGS.CAPTURE_MODE = 'sys'\n",
    "ex.captured_out_filter = apply_backspaces_and_linefeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "lr = 0.0001\n",
    "momentum = 0.\n",
    "nesterov = False\n",
    "weight_decay = 5e-4\n",
    "optim = 'adamw'\n",
    "scheduler = 'multistep'\n",
    "max_norm = 0.0\n",
    "seed = 0\n",
    "\n",
    "visdom_port = None\n",
    "visdom_freq = 100\n",
    "cpu = False  # Force training on CPU\n",
    "cudnn_flag = 'benchmark'\n",
    "temp_dir = osp.join('outputs', 'temp')\n",
    "\n",
    "no_bias_decay = False\n",
    "loss = 'bce'\n",
    "scheduler_tau = [16, 18]\n",
    "scheduler_gamma = 0.1\n",
    "\n",
    "resume = '/mnt/beegfs/home/smessoud/RerankingTransformer/RRT_GLD/rrt_gld_ckpts/r50_gldv1.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,\n",
       " False,\n",
       " 'benchmark',\n",
       " None,\n",
       " 100,\n",
       " 'outputs/temp',\n",
       " 0,\n",
       " False,\n",
       " 0.0,\n",
       " '/mnt/beegfs/home/smessoud/RerankingTransformer/RRT_GLD/rrt_gld_ckpts/r50_gldv1.pt')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs, cpu, cudnn_flag, visdom_port, visdom_freq, temp_dir, seed, no_bias_decay, max_norm, resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_scheduler(parameters, optim, loader_length, epochs, lr, momentum, nesterov, weight_decay, scheduler, scheduler_tau, scheduler_gamma, lr_step=None):\n",
    "    if optim == 'sgd':\n",
    "        optimizer = SGD(parameters, lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=True if nesterov and momentum else False)\n",
    "    elif optim == 'adam':\n",
    "        optimizer = Adam(parameters, lr=lr, weight_decay=weight_decay) \n",
    "    else:\n",
    "        optimizer = AdamW(parameters, lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    if epochs == 0:\n",
    "        scheduler = None\n",
    "        update_per_iteration = None\n",
    "    elif scheduler == 'cos':\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs * loader_length, eta_min=0.000005)\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0.000001)\n",
    "        update_per_iteration = False\n",
    "    elif scheduler == 'warmcos':\n",
    "        # warm_cosine = lambda i: min((i + 1) / 3, (1 + math.cos(math.pi * i / (epochs * loader_length))) / 2)\n",
    "        warm_cosine = lambda i: min((i + 1) / 3, (1 + math.cos(math.pi * i / epochs)) / 2)\n",
    "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=warm_cosine)\n",
    "        update_per_iteration = False\n",
    "    elif scheduler == 'multistep':\n",
    "        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=scheduler_tau, gamma=scheduler_gamma)\n",
    "        update_per_iteration = False\n",
    "    elif scheduler == 'warmstep':\n",
    "        warm_step = lambda i: min((i + 1) / 100, 1) * 0.1 ** (i // (lr_step * loader_length))\n",
    "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=warm_step)\n",
    "        update_per_iteration = True\n",
    "    else:\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, epochs * loader_length)\n",
    "        update_per_iteration = True\n",
    "\n",
    "    return optimizer, (scheduler, update_per_iteration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(loss):\n",
    "    if loss == 'bce':\n",
    "        return BinaryCrossEntropyWithLogits()\n",
    "    else:\n",
    "        raise Exception('Unsupported loss {}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() and not cpu else 'cpu')\n",
    "# callback = VisdomLogger(port=visdom_port) if visdom_port else None\n",
    "if cudnn_flag == 'deterministic':\n",
    "    setattr(cudnn, cudnn_flag, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'tuto_viquae_tuto_r50_gldv1'\n",
    "set_name = 'tuto'\n",
    "train_txt = ('tuto_query.txt', 'tuto_gallery.txt')\n",
    "test_txt = ('tuto_query.txt', 'tuto_selection.txt')\n",
    "train_data_dir = 'data/viquae_for_rrt'\n",
    "test_data_dir  = 'data/viquae_for_rrt'\n",
    "test_gnd_file = 'gnd_tuto.pkl'\n",
    "train_gnd_file = 'training_gnd_'+set_name+'.pkl'\n",
    "#train_gnd_file = 'gnd_'+set_name+'.pkl'\n",
    "desc_name = 'r50_gldv1'\n",
    "sampler = 'triplet'\n",
    "split_char  = ';;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sets(desc_name, \n",
    "        train_data_dir, test_data_dir, \n",
    "        train_txt, test_txt, train_gnd_file,\n",
    "        test_gnd_file, max_sequence_len,\n",
    "        split_char):\n",
    "    ####################################################################################################################################\n",
    "    train_gnd_data  = None if train_gnd_file is None else pickle_load(osp.join(train_data_dir, train_gnd_file))\n",
    "    train_lines     = read_file(osp.join(train_data_dir, train_txt[1]))\n",
    "    train_q_lines   = read_file(osp.join(train_data_dir, train_txt[0]))\n",
    "    train_samples   = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in train_lines]\n",
    "    train_q_samples = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in train_q_lines]\n",
    "    train_set       = FeatureDataset(train_data_dir, train_samples,   desc_name, max_sequence_len, gnd_data=train_gnd_data)\n",
    "    query_train_set = FeatureDataset(train_data_dir, train_q_samples, desc_name, max_sequence_len, gnd_data=train_gnd_data)\n",
    "    ####################################################################################################################################\n",
    "    test_gnd_data = None if test_gnd_file is None else pickle_load(osp.join(test_data_dir, test_gnd_file))\n",
    "    query_lines   = read_file(osp.join(test_data_dir, test_txt[0]))\n",
    "    gallery_lines = read_file(osp.join(test_data_dir, test_txt[1]))\n",
    "    query_samples   = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in query_lines]\n",
    "    gallery_samples = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in gallery_lines]\n",
    "    gallery_set = FeatureDataset(test_data_dir, gallery_samples, desc_name, max_sequence_len)\n",
    "    query_set   = FeatureDataset(test_data_dir, query_samples,   desc_name, max_sequence_len, gnd_data=test_gnd_data)\n",
    "        \n",
    "    return (train_set, query_train_set), (query_set, gallery_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(desc_name, train_data_dir, \n",
    "    batch_size, test_batch_size, \n",
    "    num_workers, pin_memory, \n",
    "    sampler, recalls, set_name,\n",
    "    num_candidates=100,):\n",
    "\n",
    "    (train_set, query_train_set), (query_set, gallery_set) = get_sets(\n",
    "        desc_name=desc_name, train_data_dir=train_data_dir, \n",
    "        test_data_dir=train_data_dir, train_txt=train_txt, \n",
    "        test_txt=test_txt, train_gnd_file=train_gnd_file, \n",
    "        test_gnd_file=test_gnd_file, \n",
    "        max_sequence_len=max_sequence_len, \n",
    "        split_char=split_char)\n",
    "\n",
    "    if sampler == 'random':\n",
    "        train_sampler = BatchSampler(RandomSampler(train_set), batch_size=batch_size, drop_last=False)\n",
    "    elif sampler == 'triplet':\n",
    "        #s_name = set_name\n",
    "        #if s_name != '':\n",
    "        #    s_name = set_name + '_'\n",
    "        #def map_nnids_labels(train_data_dir, train_gnd_file, s_categories):\n",
    "        #    gnd =  pickle_load(osp.join(train_data_dir, train_gnd_file))\n",
    "        #    selection_gallery = gnd['simlist']\n",
    "        #    s_categories = s_categories.reshape(np.array(selection_gallery).shape)\n",
    "        #    selection_ids_to_cat_dict = [{k: s_categories[i][k] for k in range(len(selection_gallery[i]))} for i in range(len(selection_gallery))]\n",
    "        #    return selection_ids_to_cat_dict\n",
    "        #s_path = train_data_dir+'/'+set_name+'_s_categories.txt'\n",
    "        #print('s_path: ', s_path)\n",
    "        #s_categories = np.loadtxt(s_path, dtype='int64')\n",
    "        #print('s_categories: ', s_categories.shape)\n",
    "        #map_nnids_labels = map_nnids_labels(train_data_dir, train_gnd_file, s_categories)\n",
    "        train_nn_inds = osp.join(train_data_dir, set_name + '_nn_inds_%s.pkl'%desc_name)\n",
    "        gnd_data = train_set.gnd_data['gnd']\n",
    "        train_sampler = TripletSampler(query_train_set.targets, batch_size, train_nn_inds, num_candidates, gnd_data)\n",
    "    else:\n",
    "        raise ValueError('Invalid choice of sampler ({}).'.format(sampler))\n",
    "    train_loader = DataLoader(train_set, batch_sampler=train_sampler, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    query_train_loader = DataLoader(query_train_set, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)\n",
    "        \n",
    "    query_loader   = DataLoader(query_set, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    gallery_loader = DataLoader(gallery_set, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    return MetricLoaders(train=train_loader, query_train=query_train_loader, query=query_loader, gallery=gallery_loader, num_classes=len(train_set.categories),set_name=set_name), recalls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '/mnt/beegfs/home/smessoud/RerankingTransformer/models/research/delf/delf/python/delg/data/viquae_for_rrt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLoaders(NamedTuple):\n",
    "    train: DataLoader\n",
    "    num_classes: int\n",
    "    query: DataLoader\n",
    "    query_train: DataLoader\n",
    "    set_name: str = ''\n",
    "    gallery: Optional[DataLoader] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size      = 32\n",
    "test_batch_size = 32\n",
    "max_sequence_len = 500\n",
    "num_workers = 8  # number of workers used ot load the data\n",
    "pin_memory  = True  # use the pin_memory option of DataLoader \n",
    "num_candidates = 100\n",
    "recalls = [1, 5, 6, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletSampler():\n",
    "    def __init__(self, labels, batch_size, nn_inds_path, num_candidates, gnd_data, min_pos=5):\n",
    "        self.batch_size     = batch_size\n",
    "        self.num_candidates = num_candidates\n",
    "        self.cache_nn_inds  = pickle_load(nn_inds_path)\n",
    "        self.labels = labels\n",
    "        self.gnd_data = gnd_data\n",
    "        print('nn_inds_path: ', nn_inds_path)\n",
    "        print('labels len: ', len(labels))\n",
    "        assert (len(self.cache_nn_inds) == len(labels))\n",
    "        #############################################################################\n",
    "        ## Collect valid tuples\n",
    "        valids = np.zeros_like(labels)\n",
    "        for i in range(len(self.cache_nn_inds)):\n",
    "            #nnids = self.cache_nn_inds[i]\n",
    "            #query_label = labels[i]\n",
    "            #index_labels = np.array([map_nnids_labels[i][j] for j in nnids])\n",
    "            #index_labels = np.array([labels[j] for j in nnids])\n",
    "            #positives = np.where(index_labels == query_label)[0]\n",
    "            positives = self.gnd_data[i]['r_easy']\n",
    "            if len(positives) < min_pos:\n",
    "                continue\n",
    "            valids[i] = 1\n",
    "        self.valids = np.where(valids > 0)[0]\n",
    "        self.num_samples = len(self.valids)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        cands = torch.randperm(self.num_samples).tolist()\n",
    "        for i in range(len(cands)):\n",
    "            query_idx = self.valids[cands[i]]\n",
    "            anchor_idx = self.gnd_data[query_idx]['anchor_idx']\n",
    "            #nnids = self.cache_nn_inds[anchor_idx]\n",
    "\n",
    "            positive_inds = self.gnd_data[query_idx]['g_easy']\n",
    "            negative_inds = self.gnd_data[query_idx]['g_junk']\n",
    "            assert(len(positive_inds) > 0)\n",
    "            assert(len(negative_inds) > 0)\n",
    "\n",
    "            random.shuffle(positive_inds)\n",
    "            random.shuffle(negative_inds)\n",
    "\n",
    "            batch.append(anchor_idx)\n",
    "            batch.append(positive_inds[0]) \n",
    "            batch.append(negative_inds[0])\n",
    "\n",
    "            if len(batch) >= self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                \n",
    "        if len(batch) > 0:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.num_samples * 3 + self.batch_size - 1) // self.batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1190, 100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_inds_path = '/mnt/beegfs/home/smessoud/RerankingTransformer/models/research/delf/delf/python/delg/data/viquae_for_rrt/train_nn_inds_r50_gldv1.pkl'\n",
    "cache_nn_inds  = pickle_load(nn_inds_path)\n",
    "cache_nn_inds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn_inds_path:  /mnt/beegfs/home/smessoud/RerankingTransformer/models/research/delf/delf/python/delg/data/viquae_for_rrt/tuto_nn_inds_r50_gldv1.pkl\n",
      "labels len:  100\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "loaders, recall_ks = get_loaders('r50_gldv1', train_data_dir, \n",
    "    batch_size, test_batch_size, \n",
    "    num_workers, pin_memory, \n",
    "    sampler, recalls, set_name,\n",
    "    num_candidates=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_global_features, num_local_features, seq_len, dim_K, dim_feedforward, nhead, num_encoder_layers, dropout, activation, normalize_before):\n",
    "    return MatchERT(d_global=num_global_features, d_model=num_local_features, seq_len=seq_len, d_K=dim_K, nhead=nhead, num_encoder_layers=num_encoder_layers, \n",
    "            dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, normalize_before=normalize_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'rrt'\n",
    "num_global_features = 2048  \n",
    "num_local_features = 128  \n",
    "seq_len = 1004\n",
    "dim_K = 256\n",
    "dim_feedforward = 1024\n",
    "nhead = 4\n",
    "num_encoder_layers = 6\n",
    "dropout = 0.0 \n",
    "activation = \"relu\"\n",
    "normalize_before = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed+1)\n",
    "model = get_model(num_global_features,num_local_features,seq_len,dim_K,dim_feedforward,nhead,num_encoder_layers,dropout,activation,normalize_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of trainable parameters:  2243201\n"
     ]
    }
   ],
   "source": [
    "if resume is not None:\n",
    "    checkpoint = torch.load(resume, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['state'], strict=True)\n",
    "print('# of trainable parameters: ', num_of_trainable_params(model))\n",
    "class_loss = get_loss(loss)\n",
    "nn_inds_path = osp.join(loaders.query.dataset.data_dir, loaders.set_name + '_nn_inds_%s.pkl'%loaders.query.dataset.desc_name)\n",
    "cache_nn_inds = torch.from_numpy(pickle_load(nn_inds_path)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed+2)\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)\n",
    "parameters = []\n",
    "if no_bias_decay:\n",
    "    parameters.append({'params': [par for par in model.parameters() if par.dim() != 1]})\n",
    "    parameters.append({'params': [par for par in model.parameters() if par.dim() == 1], 'weight_decay': 0})\n",
    "else:\n",
    "    parameters.append({'params': model.parameters()})\n",
    "optimizer, scheduler = get_optimizer_scheduler(parameters=parameters, loader_length=len(loaders.train),\n",
    "                                               optim=optim, epochs=epochs, lr=lr,\n",
    "                                               momentum=momentum, nesterov=nesterov, weight_decay=weight_decay,\n",
    "                                               scheduler=scheduler, scheduler_tau=scheduler_tau, \n",
    "                                               scheduler_gamma=scheduler_gamma, lr_step=None)\n",
    "if resume is not None and checkpoint.get('optim', None) is not None:\n",
    "    optimizer.load_state_dict(checkpoint['optim'])\n",
    "    del checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed+3)\n",
    "# setup partial function to simplify call\n",
    "eval_function = partial(evaluate_viquae, model=model, \n",
    "    cache_nn_inds=cache_nn_inds,\n",
    "    recall=recall_ks, query_loader=loaders.query, gallery_loader=loaders.gallery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:59<00:00,  1.67it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': 8.78, 'map@1': 2.07, 'map@5': 4.25, 'map@6': 4.53, 'mrr': 13.09, 'mrr@1': 11.0, 'mrr@5': 12.75, 'mrr@6': 12.92, 'precision': 2.18, 'precision@1': 11.0, 'precision@5': 8.2, 'precision@6': 7.83}\n",
      "{'map': 8.78,\n",
      " 'map@1': 2.07,\n",
      " 'map@5': 4.25,\n",
      " 'map@6': 4.53,\n",
      " 'mrr': 13.09,\n",
      " 'mrr@1': 11.0,\n",
      " 'mrr@5': 12.75,\n",
      " 'mrr@6': 12.92,\n",
      " 'precision': 2.18,\n",
      " 'precision@1': 11.0,\n",
      " 'precision@5': 8.2,\n",
      " 'precision@6': 7.83}\n"
     ]
    }
   ],
   "source": [
    "result = eval_function()\n",
    "pprint(result)\n",
    "best_val = (0, result, deepcopy(model.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2e6def7810>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed+4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-20fd598bbcc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m save_name = osp.join(temp_dir, '{}_{}.pt'.format(ex.current_run.config['model']['name'],\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                          ex.current_run.config['dataset']['name']))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "save_name = osp.join(temp_dir, '{}_{}.pt'.format(ex.current_run.config['model']['name'],\n",
    "                                                         ex.current_run.config['dataset']['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   [000]: 100%|███████████████████████████| 4/4 [00:02<00:00,  1.39it/s]\n",
      "100%|██████████| 100/100 [01:00<00:00,  1.65it/s]                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': 8.8, 'map@1': 2.2, 'map@5': 4.34, 'map@6': 4.5, 'mrr': 13.53, 'mrr@1': 12.0, 'mrr@5': 13.33, 'mrr@6': 13.33, 'precision': 2.18, 'precision@1': 12.0, 'precision@5': 8.0, 'precision@6': 7.33}\n",
      "Validation [000]\n",
      "{'map': 8.8,\n",
      " 'map@1': 2.2,\n",
      " 'map@5': 4.34,\n",
      " 'map@6': 4.5,\n",
      " 'mrr': 13.53,\n",
      " 'mrr@1': 12.0,\n",
      " 'mrr@5': 13.33,\n",
      " 'mrr@6': 13.33,\n",
      " 'precision': 2.18,\n",
      " 'precision@1': 12.0,\n",
      " 'precision@5': 8.0,\n",
      " 'precision@6': 7.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    if cudnn_flag == 'benchmark':\n",
    "        setattr(cudnn, cudnn_flag, True)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    train_one_epoch(model=model, loader=loaders.train, class_loss=class_loss, optimizer=optimizer, scheduler=scheduler, max_norm=max_norm, epoch=epoch, freq=visdom_freq, ex=None)\n",
    "    \n",
    "    # validation\n",
    "    if cudnn_flag == 'benchmark':\n",
    "        setattr(cudnn, cudnn_flag, False)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    result = eval_function()\n",
    "    \n",
    "    print('Validation [{:03d}]'.format(epoch)), pprint(result)\n",
    "    #ex.log_scalar('val.map', result['map'], step=epoch + 1)\n",
    "\n",
    "    if result['map'] >= best_val[1]['map']:\n",
    "        print('New best model in epoch %d.'%epoch)\n",
    "        best_val = (epoch + 1, result, deepcopy(model.state_dict()))\n",
    "        #torch.save({'state': state_dict_to_cpu(best_val[2]), 'optim': optimizer.state_dict()}, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnd =  pickle_load(osp.join(train_data_dir, train_gnd_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['easy', 'hard', 'junk', 'neg', 'provenance_entity', 'img_rank_dict', 'rank_img_dict', 'r_easy', 'r_hard', 'r_junk', 'r_neg', 'anchor_idx', 'g_easy', 'g_hard', 'g_junk', 'g_neg'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnd['gnd'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines     = read_file(osp.join(train_data_dir, train_txt))\n",
    "train_samples   = [(line.split(split_char)[0], int(line.split(split_char)[1]), int(line.split(split_char)[2]), int(line.split(split_char)[3])) for line in train_lines]\n",
    "train_set       = FeatureDataset(train_data_dir, train_samples, desc_name, max_sequence_len)\n",
    "query_train_set = FeatureDataset(train_data_dir, train_samples, desc_name, max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s_name = set_name\n",
    "#if s_name != '':\n",
    "#    s_name = set_name + '_'\n",
    "#def map_nnids_labels(train_data_dir, train_gnd_file, s_categories):\n",
    "#    gnd =  pickle_load(osp.join(train_data_dir, train_gnd_file))\n",
    "#    selection_gallery = gnd['simlist']\n",
    "#    s_categories = s_categories.reshape(np.array(selection_gallery).shape)\n",
    "#    selection_ids_to_cat_dict = [{k: s_categories[i][k] for k in range(len(selection_gallery[i]))} for i in range(len(selection_gallery))]\n",
    "#    print(s_categories.shape)\n",
    "#\n",
    "#    return selection_ids_to_cat_dict\n",
    "#\n",
    "#s_path = train_data_dir+'/'+set_name+'_s_categories.txt'\n",
    "#s_categories = np.loadtxt(s_path, dtype='int64')\n",
    "#map_nnids_labels = map_nnids_labels(train_data_dir, train_gnd_file, s_categories)\n",
    "\n",
    "train_nn_inds = osp.join(train_data_dir, set_name + '_nn_inds_%s.pkl'%desc_name)\n",
    "train_sampler = TripletSampler(train_set.targets, batch_size, train_nn_inds, num_candidates, gnd['gnd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn_inds_path =  osp.join(train_data_dir, 'training_' + s_name+'nn_inds_%s.pkl'%desc_name)\n",
    "#cache_nn_inds = torch.from_numpy(pickle_load(nn_inds_path)).long()\n",
    "#cache_nn_inds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_sampler.valids), np.where(train_sampler.valids > 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_sampler:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnd['gnd'][10]['hard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnd['simlist'][1][6:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
