{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from pprint import pprint\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacred\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.backends import cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, BatchSampler\n",
    "from typing import NamedTuple, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.matcher import MatchERT\n",
    "from models.ingredient import model_ingredient, get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacred import SETTINGS\n",
    "from sacred.utils import apply_backspaces_and_linefeeds\n",
    "from sacred import Ingredient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/beegfs/home/smessoud/anaconda3/envs/rrt/lib/python3.7/site-packages/ranx/qrels_run_common.py:7: UserWarning: Sorting disabled. Assumes that you provided sorted doc_ids!\n",
      "  warnings.warn(\"Sorting disabled. Assumes that you provided sorted doc_ids!\")\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import pickle_load\n",
    "from sacred import Experiment\n",
    "from utils.data.dataset_ingredient import data_ingredient, get_loaders\n",
    "from utils.data.dataset import FeatureDataset\n",
    "ex = sacred.Experiment('RRT Evaluation', ingredients=[data_ingredient, model_ingredient], interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = sacred.Experiment('Prepare Top-K (VIQUAE FOR RTT)', interactive=True)\n",
    "# Filter backspaces and linefeeds\n",
    "SETTINGS.CAPTURE_MODE = 'sys'\n",
    "ex.captured_out_filter = apply_backspaces_and_linefeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = 'r50_gldv1'\n",
    "set_name = 'tuto'\n",
    "gnd_name = 'gnd_'+ set_name+'.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = False\n",
    "cudnn_flag = 'benchmark'\n",
    "temp_dir = osp.join('logs', 'temp')\n",
    "resume = '/mnt/beegfs/home/smessoud/RerankingTransformer/RRT_GLD/rrt_gld_ckpts/r50_gldv1.pt'\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'viquae_for_rrt'\n",
    "data_dir = osp.join('/mnt/beegfs/home/smessoud/RerankingTransformer/models/research/delf/delf/python/delg/data', dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8ef69927f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() and not cpu else 'cpu')\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cudnn_flag == 'deterministic':\n",
    "    setattr(cudnn, cudnn_flag, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sets(desc_name, \n",
    "        train_data_dir, test_data_dir, \n",
    "        train_txt, test_txt, test_gnd_file, \n",
    "        max_sequence_len=500):\n",
    "    ####################################################################################################################################\n",
    "    train_lines     = read_file(osp.join(train_data_dir, train_txt))\n",
    "    train_samples   = [(line.split(';;')[0], int(line.split(';;')[1]), int(line.split(';;')[2]), int(line.split(';;')[3])) for line in train_lines]\n",
    "    train_set       = FeatureDataset(train_data_dir, train_samples, desc_name, max_sequence_len)\n",
    "    query_train_set = FeatureDataset(train_data_dir, train_samples, desc_name, max_sequence_len)\n",
    "    ####################################################################################################################################\n",
    "    test_gnd_data = None if test_gnd_file is None else pickle_load(osp.join(test_data_dir, test_gnd_file))\n",
    "    query_lines   = read_file(osp.join(test_data_dir, test_txt[0]))\n",
    "    gallery_lines = read_file(osp.join(test_data_dir, test_txt[1]))\n",
    "    query_samples   = [(line.split(';;')[0], int(line.split(';;')[1]), int(line.split(';;')[2]), int(line.split(';;')[3])) for line in query_lines]\n",
    "    gallery_samples = [(line.split(';;')[0], int(line.split(';;')[1]), int(line.split(';;')[2]), int(line.split(';;')[3])) for line in gallery_lines]\n",
    "    gallery_set = FeatureDataset(test_data_dir, gallery_samples, desc_name, max_sequence_len)\n",
    "    query_set   = FeatureDataset(test_data_dir, query_samples,   desc_name, max_sequence_len, gnd_data=test_gnd_data)\n",
    "        \n",
    "    return (train_set, query_train_set), (query_set, gallery_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLoaders(NamedTuple):\n",
    "    train: DataLoader\n",
    "    num_classes: int\n",
    "    query: DataLoader\n",
    "    query_train: DataLoader\n",
    "    gallery: Optional[DataLoader] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(desc_name, train_data_dir, \n",
    "    batch_size=8, test_batch_size=8, \n",
    "    num_workers=8, pin_memory=True, \n",
    "    sampler='random', recalls=[1, 5, 10],\n",
    "    num_candidates=100):\n",
    "\n",
    "    (train_set, query_train_set), (query_set, gallery_set) = get_sets(desc_name, \n",
    "        train_data_dir=train_data_dir,\n",
    "        test_data_dir=train_data_dir,\n",
    "        train_txt='tuto_query.txt',\n",
    "        test_txt=('tuto_query.txt', 'tuto_selection.txt'),\n",
    "        test_gnd_file='gnd_tuto.pkl', \n",
    "        max_sequence_len=500)\n",
    "\n",
    "    if sampler == 'random':\n",
    "        train_sampler = BatchSampler(RandomSampler(train_set), batch_size=batch_size, drop_last=False)\n",
    "    elif sampler == 'triplet':\n",
    "        train_nn_inds = osp.join(train_data_dir, set_name+'_nn_inds_%s.pkl'%desc_name)\n",
    "        train_sampler = TripletSampler(train_set.targets, batch_size, train_nn_inds, num_candidates)\n",
    "    else:\n",
    "        raise ValueError('Invalid choice of sampler ({}).'.format(sampler))\n",
    "    train_loader = DataLoader(train_set, batch_sampler=train_sampler, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    query_train_loader = DataLoader(query_train_set, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)\n",
    "        \n",
    "    query_loader   = DataLoader(query_set, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    gallery_loader = DataLoader(gallery_set, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    return MetricLoaders(train=train_loader, query_train=query_train_loader, query=query_loader, gallery=gallery_loader, num_classes=len(train_set.categories)), recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '/mnt/beegfs/home/smessoud/RerankingTransformer/models/research/delf/delf/python/delg/data/viquae_for_rrt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_txt = set_name+'_query.txt'\n",
    "#query_lines     = read_file(osp.join(train_data_dir, train_txt))\n",
    "#train_samples   = [(line.split(';;')[0], int(line.split(';;')[1]), int(line.split(';;')[2]), int(line.split(';;')[3])) for line in train_lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_txt = ('test_query.txt', 'test_selection.txt')\n",
    "#desc_name = 'r50_gldv1'\n",
    "#query_lines   = read_file(osp.join(train_data_dir, test_txt[0]))\n",
    "#gallery_lines = read_file(osp.join(train_data_dir, test_txt[1]))\n",
    "#query_samples   = [(line.split(';;')[0], int(line.split(';;')[1]), int(line.split(';;')[2]), int(line.split(';;')[3])) for line in query_lines]\n",
    "#gallery_samples = [(line.split(';;')[0], int(line.split(';;')[1]), int(line.split(';;')[2]), int(line.split(';;')[3])) for line in gallery_lines]\n",
    "#gallery_set = FeatureDataset(train_data_dir, gallery_samples, desc_name, max_sequence_len)\n",
    "#query_set   = FeatureDataset(train_data_dir, query_samples,   desc_name, max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gallery_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set, query_train_set), (query_set, gallery_set) = get_sets('r50_gldv1',\n",
    "            train_data_dir,\n",
    "            train_data_dir,\n",
    "            set_name+'_query.txt',\n",
    "            (set_name+'_query.txt',set_name+'_selection.txt'),\n",
    "            'gnd_'+set_name+'.pkl',\n",
    "            500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size      = 1\n",
    "test_batch_size = 1\n",
    "max_sequence_len = 100\n",
    "sampler = 'random'\n",
    "if sampler == 'random':\n",
    "   train_sampler = BatchSampler(RandomSampler(train_set), batch_size=batch_size, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8  # number of workers used ot load the data\n",
    "pin_memory  = True  # use the pin_memory option of DataLoader \n",
    "num_candidates = 100\n",
    "recalls = [1, 5, 6, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_sampler=train_sampler, num_workers=num_workers, pin_memory=pin_memory)\n",
    "query_train_loader = DataLoader(query_train_set, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_loader   = DataLoader(query_set, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)\n",
    "gallery_loader = DataLoader(gallery_set, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders, recall_ks = get_loaders(desc_name='r50_gldv1',\n",
    "    train_data_dir='/mnt/beegfs/home/smessoud/RerankingTransformer/models/research/delf/delf/python/delg/data/viquae_for_rrt', \n",
    "    batch_size=36, test_batch_size=36, \n",
    "    num_workers=8, pin_memory=True, \n",
    "    sampler='random', recalls=[1, 5, 10],\n",
    "    num_candidates=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ingredient = Ingredient('model', interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_global_features, num_local_features, seq_len, dim_K, dim_feedforward, nhead, num_encoder_layers, dropout, activation, normalize_before):\n",
    "    return MatchERT(d_global=num_global_features, d_model=num_local_features, seq_len=seq_len, d_K=dim_K, nhead=nhead, num_encoder_layers=num_encoder_layers, \n",
    "            dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, normalize_before=normalize_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'rrt'\n",
    "num_global_features = 2048  \n",
    "num_local_features = 128  \n",
    "seq_len = 1004\n",
    "dim_K = 256\n",
    "dim_feedforward = 1024\n",
    "nhead = 4\n",
    "num_encoder_layers = 6\n",
    "dropout = 0.0 \n",
    "activation = \"relu\"\n",
    "normalize_before = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(num_global_features,num_local_features,seq_len,dim_K,dim_feedforward,nhead,num_encoder_layers,dropout,activation,normalize_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume is not None:\n",
    "   checkpoint = torch.load(resume, map_location=torch.device('cpu'))\n",
    "   model.load_state_dict(checkpoint['state'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatchERT(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (remap): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (scale_encoder): Embedding(7, 128)\n",
       "  (seg_encoder): Embedding(6, 128)\n",
       "  (classifier): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('r50_gldv1',\n",
       " 'r50_gldv1',\n",
       " '/mnt/beegfs/home/smessoud/RerankingTransformer/models/research/delf/delf/python/delg/data/viquae_for_rrt')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders.query.dataset.desc_name, loaders.query.dataset.desc_name, loaders.query.dataset.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f8e98b683d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders.query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_inds_path = osp.join(loaders.query.dataset.data_dir, set_name+'_nn_inds_%s.pkl'%loaders.query.dataset.desc_name)\n",
    "cache_nn_inds = torch.from_numpy(pickle_load(nn_inds_path)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_nn_inds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples, top_k = cache_nn_inds.size()\n",
    "top_k = min(top_k, 100)\n",
    "top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "        model: nn.Module,\n",
    "        cache_nn_inds: torch.Tensor,\n",
    "        query_loader: DataLoader,\n",
    "        gallery_loader: DataLoader,\n",
    "        recall: List[int]):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    to_device = lambda x: x.to(device, non_blocking=True)\n",
    "\n",
    "    query_global, query_local, query_mask, query_scales, query_positions, query_names = [], [], [], [], [], []\n",
    "    gallery_global, gallery_local, gallery_mask, gallery_scales, gallery_positions, gallery_names = [], [], [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for entry in tqdm(query_loader, desc='Extracting query features', leave=False, ncols=80):\n",
    "            q_global, q_local, q_mask, q_scales, q_positions, _, q_names = entry\n",
    "            query_global.append(q_global.cpu())\n",
    "            query_local.append(q_local.cpu())\n",
    "            query_mask.append(q_mask.cpu())\n",
    "            query_scales.append(q_scales.cpu())\n",
    "            query_positions.append(q_positions.cpu())\n",
    "            query_names.extend(list(q_names))\n",
    "\n",
    "        query_global    = torch.cat(query_global, 0)\n",
    "        query_local     = torch.cat(query_local, 0)\n",
    "        query_mask      = torch.cat(query_mask, 0)\n",
    "        query_scales    = torch.cat(query_scales, 0)\n",
    "        query_positions = torch.cat(query_positions, 0)\n",
    "\n",
    "        for entry in tqdm(gallery_loader, desc='Extracting gallery features', leave=False, ncols=80):\n",
    "            g_global, g_local, g_mask, g_scales, g_positions, _, g_names = entry\n",
    "            gallery_global.append(g_global.cpu())\n",
    "            gallery_local.append(g_local.cpu())\n",
    "            gallery_mask.append(g_mask.cpu())\n",
    "            gallery_scales.append(g_scales.cpu())\n",
    "            gallery_positions.append(g_positions.cpu())\n",
    "            gallery_names.extend(list(g_names))\n",
    "            \n",
    "        gallery_global    = torch.cat(gallery_global, 0)\n",
    "        gallery_local     = torch.cat(gallery_local, 0)\n",
    "        gallery_mask      = torch.cat(gallery_mask, 0)\n",
    "        gallery_scales    = torch.cat(gallery_scales, 0)\n",
    "        gallery_positions = torch.cat(gallery_positions, 0)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        evaluate_function = partial(mean_average_precision_revisited_rerank, model=model, cache_nn_inds=cache_nn_inds,\n",
    "            query_global=query_global, query_local=query_local, query_mask=query_mask, query_scales=query_scales, query_positions=query_positions, \n",
    "            gallery_global=gallery_global, gallery_local=gallery_local, gallery_mask=gallery_mask, gallery_scales=gallery_scales, gallery_positions=gallery_positions, \n",
    "            ks=recall, \n",
    "            gnd=query_loader.dataset.gnd_data,\n",
    "        )\n",
    "        metrics = evaluate_function()\n",
    "    return metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "to_device = lambda x: x.to(device, non_blocking=True)\n",
    "\n",
    "query_global, query_local, query_mask, query_scales, query_positions, query_names = [], [], [], [], [], []\n",
    "gallery_global, gallery_local, gallery_mask, gallery_scales, gallery_positions, gallery_names = [], [], [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for entry in tqdm(query_loader, desc='Extracting query features', leave=False, ncols=80):\n",
    "        q_global, q_local, q_mask, q_scales, q_positions, _, q_names = entry\n",
    "        query_global.append(q_global.cpu())\n",
    "        query_local.append(q_local.cpu())\n",
    "        query_mask.append(q_mask.cpu())\n",
    "        query_scales.append(q_scales.cpu())\n",
    "        query_positions.append(q_positions.cpu())\n",
    "        query_names.extend(list(q_names))\n",
    "\n",
    "    query_global    = torch.cat(query_global, 0)\n",
    "    query_local     = torch.cat(query_local, 0)\n",
    "    query_mask      = torch.cat(query_mask, 0)\n",
    "    query_scales    = torch.cat(query_scales, 0)\n",
    "    query_positions = torch.cat(query_positions, 0)\n",
    "    \n",
    "    for entry in tqdm(gallery_loader, desc='Extracting gallery features', leave=False, ncols=80):\n",
    "        g_global, g_local, g_mask, g_scales, g_positions, _, g_names = entry\n",
    "        gallery_global.append(g_global.cpu())\n",
    "        gallery_local.append(g_local.cpu())\n",
    "        gallery_mask.append(g_mask.cpu())\n",
    "        gallery_scales.append(g_scales.cpu())\n",
    "        gallery_positions.append(g_positions.cpu())\n",
    "        gallery_names.extend(list(g_names))\n",
    "\n",
    "    gallery_global    = torch.cat(gallery_global, 0)\n",
    "    gallery_local     = torch.cat(gallery_local, 0)\n",
    "    gallery_mask      = torch.cat(gallery_mask, 0)\n",
    "    gallery_scales    = torch.cat(gallery_scales, 0)\n",
    "    gallery_positions = torch.cat(gallery_positions, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3602432, 39845888, 36243456)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "f, r, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(model.parameters()).device\n",
    "query_global    = query_global.to(device)\n",
    "query_local     = query_local.to(device)\n",
    "query_mask      = query_mask.to(device)\n",
    "query_scales    = query_scales.to(device)\n",
    "query_positions = query_positions.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples, top_k = cache_nn_inds.size()\n",
    "top_k = min(100, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k:  100\n",
      "query_global size:  torch.Size([100, 2048])\n",
      "query_local size:  torch.Size([100, 500, 128])\n",
      "query_mask size:  torch.Size([100, 500])\n",
      "query_scales size:  torch.Size([100, 500])\n",
      "query_positions size:  torch.Size([100, 500, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"top_k: \", top_k)    \n",
    "print(\"query_global size: \", query_global.shape)\n",
    "print(\"query_local size: \", query_local.shape)\n",
    "print(\"query_mask size: \", query_mask.shape)\n",
    "print(\"query_scales size: \", query_scales.shape)\n",
    "print(\"query_positions size: \", query_positions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k:  100\n",
      "gallery_global size:  torch.Size([10000, 2048])\n",
      "gallery_local size:  torch.Size([10000, 500, 128])\n",
      "gallery_mask size:  torch.Size([10000, 500])\n",
      "gallery_scales size:  torch.Size([10000, 500])\n",
      "gallery_positions size:  torch.Size([10000, 500, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"top_k: \", top_k)    \n",
    "print(\"gallery_global size: \", gallery_global.shape)\n",
    "print(\"gallery_local size: \", gallery_local.shape)\n",
    "print(\"gallery_mask size: \", gallery_mask.shape)\n",
    "print(\"gallery_scales size: \", gallery_scales.shape)\n",
    "print(\"gallery_positions size: \", gallery_positions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery_global = gallery_global.view(query_global.size(dim=0), 100, query_global.size(dim=1))\n",
    "gallery_local = gallery_local.view(query_local.size(dim=0), 100, query_local.size(dim=1), query_local.size(dim=2))\n",
    "gallery_mask = gallery_mask.view(query_mask.size(dim=0), 100, query_mask.size(dim=1))\n",
    "gallery_scales = gallery_scales.view(query_scales.size(dim=0), 100, query_scales.size(dim=1))\n",
    "gallery_positions = gallery_positions.view(query_positions.size(dim=0), 100, query_positions.size(dim=1), query_positions.size(dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k:  100\n",
      "gallery_global size:  torch.Size([100, 100, 2048])\n",
      "gallery_local size:  torch.Size([100, 100, 500, 128])\n",
      "gallery_mask size:  torch.Size([100, 100, 500])\n",
      "gallery_scales size:  torch.Size([100, 100, 500])\n",
      "gallery_positions size:  torch.Size([100, 100, 500, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"top_k: \", top_k)    \n",
    "print(\"gallery_global size: \", gallery_global.shape)\n",
    "print(\"gallery_local size: \", gallery_local.shape)\n",
    "print(\"gallery_mask size: \", gallery_mask.shape)\n",
    "print(\"gallery_scales size: \", gallery_scales.shape)\n",
    "print(\"gallery_positions size: \", gallery_positions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   35394 KB |   35394 KB |   35394 KB |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   35394 KB |   35394 KB |   35394 KB |       0 B  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |   38912 KB |   38912 KB |   38912 KB |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |    3518 KB |    3909 KB |   11217 KB |    7699 KB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |      83    |      83    |      83    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |      83    |      83    |      83    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       7    |       7    |       7    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       6    |       6    |       7    |       1    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device='cuda:0', abbreviated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnd_data = pickle_load(osp.join(data_dir, gnd_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_nn_inds = deepcopy(cache_nn_inds.cpu().data.numpy())\n",
    "# Exclude the junk images as in DELG (https://github.com/tensorflow/models/blob/44cad43aadff9dd12b00d4526830f7ea0796c047/research/delf/delf/python/detect_to_retrieve/image_reranking.py#L190)\n",
    "for i in range(num_samples):\n",
    "    junk_ids = gnd_data['gnd'][i]['junk']\n",
    "    all_ids = eval_nn_inds[i]\n",
    "    pos = np.in1d(all_ids, junk_ids)\n",
    "    neg = np.array([not x for x in pos])\n",
    "    new_ids = np.concatenate([np.arange(len(all_ids))[neg], np.arange(len(all_ids))[pos]])\n",
    "    new_ids = all_ids[new_ids]\n",
    "    eval_nn_inds[i] = new_ids\n",
    "eval_nn_inds = torch.from_numpy(eval_nn_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nn_inds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([78, 97, 96,  0,  8, 95, 43, 98, 17, 43, 88, 63, 24, 79, 76, 50,  8, 40,\n",
       "         67, 81, 95, 26, 69, 70,  0, 71, 30, 56, 20, 17, 84,  7,  5, 73, 61, 78,\n",
       "         51,  1, 36, 99, 46, 20,  0, 65, 48, 60, 29, 43, 40, 63, 34, 20,  9, 74,\n",
       "         65, 93, 23, 76, 48, 94, 22,  2,  8, 46, 98, 47, 22, 64, 98, 29, 92, 57,\n",
       "         58, 90, 69, 30, 40, 13,  6, 15, 58, 58, 62, 59, 68, 76,  4, 67, 52, 16,\n",
       "         94, 21, 76, 94,  1, 82, 82, 73, 65, 69]),\n",
       " torch.Size([100]),\n",
       " torch.Size([100, 100, 2048]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "nnids = eval_nn_inds[:, i]\n",
    "nnids, nnids.shape, gallery_global.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2048]), 100, torch.Size([100]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_global[0].unsqueeze(dim=0).shape, nnids.size(dim=0), nnids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_scores =  []\n",
    "for iterator in range(100):\n",
    "    index_global = [gallery_global[iterator, nnids[iterator]]]\n",
    "    index_local = [gallery_local[iterator, nnids[iterator]]]\n",
    "    index_mask = [gallery_mask[iterator, nnids[iterator]]]\n",
    "    index_scales = [gallery_scales[iterator, nnids[iterator]]]\n",
    "    index_positions = [gallery_positions[iterator, nnids[iterator]]]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    index_global = torch.from_numpy(np.stack(index_global, axis=0))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    index_local = torch.from_numpy(np.stack(index_local, axis=0))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    index_mask = torch.from_numpy(np.stack(index_mask, axis=0))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    index_scales = torch.from_numpy(np.stack(index_scales, axis=0))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    index_positions = torch.from_numpy(np.stack(index_positions, axis=0))\n",
    "\n",
    "    q_global = query_global[iterator].unsqueeze(dim=0)\n",
    "    q_local = query_local[iterator].unsqueeze(dim=0)\n",
    "    q_mask = query_mask[iterator].unsqueeze(dim=0)\n",
    "    q_scales = query_scales[iterator].unsqueeze(dim=0)\n",
    "    q_postions = query_positions[iterator].unsqueeze(dim=0)\n",
    "\n",
    "    iter_scores = model(\n",
    "        q_global, q_local, q_mask, q_scales, q_positions,\n",
    "        index_global.to(device),\n",
    "        index_local.to(device),\n",
    "        index_mask.to(device),\n",
    "        index_scales.to(device),\n",
    "        index_positions.to(device))\n",
    "    \n",
    "    c_scores.append(iter_scores.cpu().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7295,  1.7295,  1.7295,  ..., -1.7395, -1.7395, -1.7395],\n",
       "        [ 1.4293, -0.2676,  0.2743,  ..., -1.9606, -1.9606, -3.6145],\n",
       "        [ 1.4293, -0.2676,  0.2743,  ..., -3.2511, -3.3540, -3.5074],\n",
       "        ...,\n",
       "        [-1.0915, -3.0135, -3.2550,  ..., -2.9478, -2.9478, -2.9478],\n",
       "        [-3.2819, -2.7184, -3.0668,  ..., -5.5008, -2.9316, -2.9316],\n",
       "        [-2.6162, -1.0304, -1.1477,  ..., -2.9478, -2.9478, -5.2138]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:03<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in tqdm(range(top_k)):\n",
    "    nnids = eval_nn_inds[:, i]\n",
    "    topk_scores =  []\n",
    "    for iterator in range(nnids.size(dim=0)):\n",
    "        index_global = [gallery_global[iterator, nnids[iterator]]]\n",
    "        index_local = [gallery_local[iterator, nnids[iterator]]]\n",
    "        index_mask = [gallery_mask[iterator, nnids[iterator]]]\n",
    "        index_scales = [gallery_scales[iterator, nnids[iterator]]]\n",
    "        index_positions = [gallery_positions[iterator, nnids[iterator]]]\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        index_global = torch.from_numpy(np.stack(index_global, axis=0))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        index_local = torch.from_numpy(np.stack(index_local, axis=0))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        index_mask = torch.from_numpy(np.stack(index_mask, axis=0))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        index_scales = torch.from_numpy(np.stack(index_scales, axis=0))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        index_positions = torch.from_numpy(np.stack(index_positions, axis=0))\n",
    "        \n",
    "        q_global = query_global[iterator].unsqueeze(dim=0)\n",
    "        q_local = query_local[iterator].unsqueeze(dim=0)\n",
    "        q_mask = query_mask[iterator].unsqueeze(dim=0)\n",
    "        q_scales = query_scales[iterator].unsqueeze(dim=0)\n",
    "        q_postions = query_positions[iterator].unsqueeze(dim=0)\n",
    "        \n",
    "        iter_scores = model(\n",
    "            q_global, q_local, q_mask, q_scales, q_positions,\n",
    "            index_global.to(device),\n",
    "            index_local.to(device),\n",
    "            index_mask.to(device),\n",
    "            index_scales.to(device),\n",
    "            index_positions.to(device))\n",
    "        \n",
    "        topk_scores.append(iter_scores.cpu().data)\n",
    "    \n",
    "    current_scores = torch.from_numpy(np.stack(topk_scores, axis=0)).squeeze(1)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    scores.append(current_scores.cpu().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnd_data = pickle_load(osp.join(data_dir, gnd_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': 18.69, 'map@1': 2.34, 'map@5': 5.6, 'map@10': 8.04, 'mrr': 23.29, 'mrr@1': 15.0, 'mrr@5': 20.83, 'mrr@10': 21.95, 'precision': 9.71, 'precision@1': 15.0, 'precision@5': 14.4, 'precision@10': 14.4}\n"
     ]
    }
   ],
   "source": [
    "scores = torch.stack(scores, -1) # nb_queries x 100\n",
    "closest_dists, indices = torch.sort(scores, dim=-1, descending=True)\n",
    "closest_indices = torch.gather(eval_nn_inds, -1, indices)\n",
    "ranks = deepcopy(eval_nn_inds)\n",
    "ranks[:, :top_k] = deepcopy(closest_indices)\n",
    "ranks = ranks.cpu().data.numpy().T\n",
    "# pickle_save('eval_nn_inds.pkl', ranks.T)\n",
    "out = compute_metrics('viquae', ranks, gnd_data['gnd'], kappas=[1,5,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4292610883712769"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_scores.cpu().data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.7295,  1.4293,  1.4293, -0.3602, -1.9409,  7.8809,  4.1849, -1.1213,\n",
       "         -2.7787, -3.7398,  2.6100, -3.0069, -4.3349,  1.2292,  7.7114,  6.0259,\n",
       "         -0.4281,  2.0132, -3.4566,  0.3535, -1.6808,  0.4280,  0.2461, -2.6675,\n",
       "         -2.9625,  4.2006, -1.7271,  2.2924, -0.5409, -0.9304,  0.5119, -1.0696,\n",
       "          7.2333, -2.6997, -0.6677, -3.1610,  8.0865,  1.6478, -1.4728,  1.0965,\n",
       "          3.3788,  7.8428,  5.6013,  0.2370,  6.0813, -0.4101, -0.0607,  1.5350,\n",
       "         -2.1898, -0.3205, -2.9534, -2.8657, -2.0753,  8.4222,  8.4184,  0.2731,\n",
       "          0.5053,  8.4222, -2.7350,  6.6646, -2.2468,  8.4184, -1.1454, -3.0600,\n",
       "         -1.1788,  5.0812, -1.3574,  6.1592,  7.6116,  8.1979, -3.5890,  4.4803,\n",
       "         -1.6283,  3.0267, -3.4154, -3.0453, -1.8392,  0.1695,  0.3817,  1.0198,\n",
       "          8.4204, -3.2018, -1.7226, -2.7281,  4.0545,  0.6906, -1.9036,  2.2423,\n",
       "          3.2200,  7.0380,  0.8479,  8.4142,  0.2687, -2.6870,  8.4194,  2.1687,\n",
       "         -3.2969, -1.0915, -3.2819, -2.6162]),\n",
       " [tensor([1.7295]),\n",
       "  tensor([1.4293]),\n",
       "  tensor([1.4293]),\n",
       "  tensor([-0.3602]),\n",
       "  tensor([-1.9409]),\n",
       "  tensor([7.8809]),\n",
       "  tensor([4.1849]),\n",
       "  tensor([-1.1213]),\n",
       "  tensor([-2.7787]),\n",
       "  tensor([-3.7398]),\n",
       "  tensor([2.6100]),\n",
       "  tensor([-3.0069]),\n",
       "  tensor([-4.3349]),\n",
       "  tensor([1.2292]),\n",
       "  tensor([7.7114]),\n",
       "  tensor([6.0259]),\n",
       "  tensor([-0.4281]),\n",
       "  tensor([2.0132]),\n",
       "  tensor([-3.4566]),\n",
       "  tensor([0.3535]),\n",
       "  tensor([-1.6808]),\n",
       "  tensor([0.4280]),\n",
       "  tensor([0.2461]),\n",
       "  tensor([-2.6675]),\n",
       "  tensor([-2.9625]),\n",
       "  tensor([4.2006]),\n",
       "  tensor([-1.7271]),\n",
       "  tensor([2.2924]),\n",
       "  tensor([-0.5409]),\n",
       "  tensor([-0.9304]),\n",
       "  tensor([0.5119]),\n",
       "  tensor([-1.0696]),\n",
       "  tensor([7.2333]),\n",
       "  tensor([-2.6997]),\n",
       "  tensor([-0.6677]),\n",
       "  tensor([-3.1610]),\n",
       "  tensor([8.0865]),\n",
       "  tensor([1.6478]),\n",
       "  tensor([-1.4728]),\n",
       "  tensor([1.0965]),\n",
       "  tensor([3.3788]),\n",
       "  tensor([7.8428]),\n",
       "  tensor([5.6013]),\n",
       "  tensor([0.2370]),\n",
       "  tensor([6.0813]),\n",
       "  tensor([-0.4101]),\n",
       "  tensor([-0.0607]),\n",
       "  tensor([1.5350]),\n",
       "  tensor([-2.1898]),\n",
       "  tensor([-0.3205]),\n",
       "  tensor([-2.9534]),\n",
       "  tensor([-2.8657]),\n",
       "  tensor([-2.0753]),\n",
       "  tensor([8.4222]),\n",
       "  tensor([8.4184]),\n",
       "  tensor([0.2731]),\n",
       "  tensor([0.5053]),\n",
       "  tensor([8.4222]),\n",
       "  tensor([-2.7350]),\n",
       "  tensor([6.6646]),\n",
       "  tensor([-2.2468]),\n",
       "  tensor([8.4184]),\n",
       "  tensor([-1.1454]),\n",
       "  tensor([-3.0600]),\n",
       "  tensor([-1.1788]),\n",
       "  tensor([5.0812]),\n",
       "  tensor([-1.3574]),\n",
       "  tensor([6.1592]),\n",
       "  tensor([7.6116]),\n",
       "  tensor([8.1979]),\n",
       "  tensor([-3.5890]),\n",
       "  tensor([4.4803]),\n",
       "  tensor([-1.6283]),\n",
       "  tensor([3.0267]),\n",
       "  tensor([-3.4154]),\n",
       "  tensor([-3.0453]),\n",
       "  tensor([-1.8392]),\n",
       "  tensor([0.1695]),\n",
       "  tensor([0.3817]),\n",
       "  tensor([1.0198]),\n",
       "  tensor([8.4204]),\n",
       "  tensor([-3.2018]),\n",
       "  tensor([-1.7226]),\n",
       "  tensor([-2.7281]),\n",
       "  tensor([4.0545]),\n",
       "  tensor([0.6906]),\n",
       "  tensor([-1.9036]),\n",
       "  tensor([2.2423]),\n",
       "  tensor([3.2200]),\n",
       "  tensor([7.0380]),\n",
       "  tensor([0.8479]),\n",
       "  tensor([8.4142]),\n",
       "  tensor([0.2687]),\n",
       "  tensor([-2.6870]),\n",
       "  tensor([8.4194]),\n",
       "  tensor([2.1687]),\n",
       "  tensor([-3.2969]),\n",
       "  tensor([-1.0915]),\n",
       "  tensor([-3.2819]),\n",
       "  tensor([-2.6162])])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(np.stack(c_scores, axis=0)).squeeze(1), c_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_global size:  torch.Size([1, 2048])\n",
      "index_local size:  torch.Size([1, 500, 128])\n",
      "index_mask size:  torch.Size([1, 500])\n",
      "index_scales size:  torch.Size([1, 500])\n",
      "index_positions size:  torch.Size([1, 500, 2])\n",
      "index_global device:  -1\n",
      "index_local device:  -1\n",
      "index_mask device:  -1\n",
      "index_scales device:  -1\n",
      "index_positions device:  -1\n",
      "query_global size:  torch.Size([1, 2048])\n",
      "query_local size:  torch.Size([1, 500, 128])\n",
      "query_mask size:  torch.Size([1, 500])\n",
      "query_scales size:  torch.Size([1, 500])\n",
      "query_positions size:  torch.Size([1, 500, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 31.75 GiB total capacity; 30.53 GiB already allocated; 4.00 MiB free; 30.60 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-529f8a32cad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mindex_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mindex_scales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             index_positions.to(device))\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m#    torch.cuda.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rrt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RerankingTransformer/RRT_GLD/models/matcher.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_global, src_local, src_mask, src_scales, src_positions, tgt_global, tgt_local, tgt_mask, tgt_scales, tgt_positions, normalize)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mtgt_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         ], 1)\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rrt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RerankingTransformer/RRT_GLD/models/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, pos)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             output = layer(output, src_mask=mask,\n\u001b[0;32m---> 39\u001b[0;31m                            src_key_padding_mask=src_key_padding_mask, pos=pos)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rrt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RerankingTransformer/RRT_GLD/models/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, pos)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pre\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RerankingTransformer/RRT_GLD/models/transformer.py\u001b[0m in \u001b[0;36mforward_post\u001b[0;34m(self, src, src_mask, src_key_padding_mask, pos)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_pos_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n\u001b[0;32m---> 78\u001b[0;31m                               key_padding_mask=src_key_padding_mask)[0]\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rrt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rrt/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    925\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rrt/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0mkey_padding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m     \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 31.75 GiB total capacity; 30.53 GiB already allocated; 4.00 MiB free; 30.60 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "topk_scores = []\n",
    "for i in tqdm(range(top_k)):\n",
    "    nnids = eval_nn_inds[:, i]\n",
    "    #iter_scores     = []\n",
    "    for iterator in range(nnids.size(dim=0)):\n",
    "        index_global = [gallery_global[iterator, nnids[iterator]]]\n",
    "        index_local = [gallery_local[iterator, nnids[iterator]]]\n",
    "        index_mask = [gallery_mask[iterator, nnids[iterator]]]\n",
    "        index_scales = [gallery_scales[iterator, nnids[iterator]]]\n",
    "        index_positions = [gallery_positions[iterator, nnids[iterator]]]\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        index_global = torch.from_numpy(np.stack(index_global, axis=0))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        index_local = torch.from_numpy(np.stack(index_local, axis=0))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        index_mask = torch.from_numpy(np.stack(index_mask, axis=0))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        index_scales = torch.from_numpy(np.stack(index_scales, axis=0))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        index_positions = torch.from_numpy(np.stack(index_positions, axis=0))\n",
    "\n",
    "        print(\"index_global size: \", index_global.shape)\n",
    "        print(\"index_local size: \", index_local.shape)\n",
    "        print(\"index_mask size: \", index_mask.shape)\n",
    "        print(\"index_scales size: \", index_scales.shape)\n",
    "        print(\"index_positions size: \", index_positions.shape)\n",
    "\n",
    "\n",
    "        print(\"index_global device: \", index_global.get_device())\n",
    "        print(\"index_local device: \", index_local.get_device())\n",
    "        print(\"index_mask device: \", index_mask.get_device())\n",
    "        print(\"index_scales device: \", index_scales.get_device())\n",
    "        print(\"index_positions device: \", index_positions.get_device())\n",
    "        \n",
    "        q_global = query_global[iterator].unsqueeze(dim=0)\n",
    "        q_local = query_local[iterator].unsqueeze(dim=0)\n",
    "        q_mask = query_mask[iterator].unsqueeze(dim=0)\n",
    "        q_scales = query_scales[iterator].unsqueeze(dim=0)\n",
    "        q_postions = query_positions[iterator].unsqueeze(dim=0)\n",
    "        \n",
    "        print(\"query_global size: \", q_global.shape)\n",
    "        print(\"query_local size: \", q_local.shape)\n",
    "        print(\"query_mask size: \", q_mask.shape)\n",
    "        print(\"query_scales size: \", q_scales.shape)\n",
    "        print(\"query_positions size: \", q_positions.shape)\n",
    "        \n",
    "        iter_scores = model(\n",
    "            q_global, q_local, q_mask, q_scales, q_positions,\n",
    "            index_global.to(device),\n",
    "            index_local.to(device),\n",
    "            index_mask.to(device),\n",
    "            index_scales.to(device),\n",
    "            index_positions.to(device))\n",
    "    \n",
    "    torch.from_numpy(np.stack(c_scores, axis=0)).squeeze(1)\n",
    "\n",
    "#    torch.cuda.empty_cache()\n",
    "#    scores.append(current_scores.cpu().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_scores.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_k:  100\n",
    "query_global size:  torch.Size([100, 2048])\n",
    "query_local size:  torch.Size([100, 500, 128])\n",
    "query_mask size:  torch.Size([100, 500])\n",
    "query_scales size:  torch.Size([100, 500])\n",
    "query_positions size:  torch.Size([100, 500, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_global size:  torch.Size([100, 2048])\n",
      "index_local size:  torch.Size([100, 500, 128])\n",
      "index_mask size:  torch.Size([100, 500])\n",
      "index_scales size:  torch.Size([100, 500])\n",
      "index_positions size:  torch.Size([100, 500, 2])\n",
      "index_global device:  0\n",
      "index_local device:  0\n",
      "index_mask device:  0\n",
      "index_scales device:  0\n",
      "index_positions device:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 1/2 [00:00<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_global size:  torch.Size([100, 2048])\n",
      "index_local size:  torch.Size([100, 500, 128])\n",
      "index_mask size:  torch.Size([100, 500])\n",
      "index_scales size:  torch.Size([100, 500])\n",
      "index_positions size:  torch.Size([100, 500, 2])\n",
      "index_global device:  0\n",
      "index_local device:  0\n",
      "index_mask device:  0\n",
      "index_scales device:  0\n",
      "index_positions device:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:01<00:01,  1.86s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 394.00 MiB (GPU 0; 31.75 GiB total capacity; 29.92 GiB already allocated; 44.00 MiB free; 30.56 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-d0dc952f29d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mindex_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mindex_scales\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         index_positions)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/rrt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RerankingTransformer/RRT_GLD/models/matcher.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_global, src_local, src_mask, src_scales, src_positions, tgt_global, tgt_local, tgt_mask, tgt_scales, tgt_positions, normalize)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mtgt_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         ], 1)\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rrt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RerankingTransformer/RRT_GLD/models/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, pos)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             output = layer(output, src_mask=mask,\n\u001b[0;32m---> 39\u001b[0;31m                            src_key_padding_mask=src_key_padding_mask, pos=pos)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rrt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RerankingTransformer/RRT_GLD/models/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, pos)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pre\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RerankingTransformer/RRT_GLD/models/transformer.py\u001b[0m in \u001b[0;36mforward_post\u001b[0;34m(self, src, src_mask, src_key_padding_mask, pos)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0msrc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rrt/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 394.00 MiB (GPU 0; 31.75 GiB total capacity; 29.92 GiB already allocated; 44.00 MiB free; 30.56 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for i in tqdm(range(2)):\n",
    "    nnids = eval_nn_inds[:, i]\n",
    "    index_global        = []\n",
    "    index_local         = []\n",
    "    index_mask          = []\n",
    "    index_scales        = []\n",
    "    index_positions     = []\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    for iterator in range(nnids.size(dim=0)):\n",
    "        index_global.append(gallery_global[iterator, nnids[iterator]])\n",
    "        index_local.append(gallery_local[iterator, nnids[iterator]])\n",
    "        index_mask.append(gallery_mask[iterator, nnids[iterator]])\n",
    "        index_scales.append(gallery_scales[iterator, nnids[iterator]])\n",
    "        index_positions.append(gallery_positions[iterator, nnids[iterator]])\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    index_global = np.stack(index_global, axis=0)\n",
    "    index_global = torch.from_numpy(index_global)\n",
    "    index_global = index_global.to(device)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    index_local = np.stack(index_local, axis=0)\n",
    "    index_local = torch.from_numpy(index_local)\n",
    "    index_local = index_local.to(device)\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    index_mask = np.stack(index_mask, axis=0)\n",
    "    index_mask = torch.from_numpy(index_mask)\n",
    "    index_mask = index_mask.to(device)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    index_scales = np.stack(index_scales, axis=0)\n",
    "    index_scales = torch.from_numpy(index_scales)\n",
    "    index_scales = index_scales.to(device)\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    index_positions = np.stack(index_positions, axis=0)\n",
    "    index_positions = torch.from_numpy(index_positions)\n",
    "    index_positions = index_positions.to(device)\n",
    "\n",
    "    print(\"index_global size: \", index_global.shape)\n",
    "    print(\"index_local size: \", index_local.shape)\n",
    "    print(\"index_mask size: \", index_mask.shape)\n",
    "    print(\"index_scales size: \", index_scales.shape)\n",
    "    print(\"index_positions size: \", index_positions.shape)\n",
    "    \n",
    "    \n",
    "    print(\"index_global device: \", index_global.get_device())\n",
    "    print(\"index_local device: \", index_local.get_device())\n",
    "    print(\"index_mask device: \", index_mask.get_device())\n",
    "    print(\"index_scales device: \", index_scales.get_device())\n",
    "    print(\"index_positions device: \", index_positions.get_device())\n",
    "    \n",
    "    current_scores = model(\n",
    "        query_global, query_local, query_mask, query_scales, query_positions,\n",
    "        index_global,\n",
    "        index_local,\n",
    "        index_mask,\n",
    "        index_scales,\n",
    "        index_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_global    = []\n",
    "for idx in range(nnids.size(dim=0)):\n",
    "    index_global.append(gallery_global[idx, nnids[idx]])\n",
    "index_global = np.stack(index_global, axis=0)\n",
    "index_global = torch.from_numpy(index_global).cuda(\"cuda:0\")\n",
    "index_local     = []\n",
    "for idx in range(nnids.size(dim=0)):\n",
    "    index_local.append(gallery_local[idx, nnids[idx]])\n",
    "index_local = np.stack(index_local, axis=0)\n",
    "index_local = torch.from_numpy(index_local).cuda(\"cuda:0\")\n",
    "index_mask     = []\n",
    "for idx in range(nnids.size(dim=0)):\n",
    "    index_mask.append(gallery_mask[idx, nnids[idx]])\n",
    "index_mask = np.stack(index_mask, axis=0)\n",
    "index_mask = torch.from_numpy(index_mask).cuda(\"cuda:0\")\n",
    "index_scales     = []\n",
    "for idx in range(nnids.size(dim=0)):\n",
    "    index_scales.append(gallery_scales[idx, nnids[idx]])\n",
    "index_scales = np.stack(index_scales, axis=0)\n",
    "index_scales = torch.from_numpy(index_scales).cuda(\"cuda:0\")\n",
    "#index_scales    = gallery_scales[:, nnids]\n",
    "index_positions     = []\n",
    "for idx in range(nnids.size(dim=0)):\n",
    "    index_positions.append(gallery_positions[idx, nnids[idx]])\n",
    "index_positions = np.stack(index_positions, axis=0)\n",
    "index_positions = torch.from_numpy(index_positions).cuda(\"cuda:0\")\n",
    "#index_positions = gallery_positions[:, nnids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_global.shape, index_local.shape, index_mask.shape, index_scales.shape, index_positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_global\n",
    "#query_local, \n",
    "#query_mask, \n",
    "#query_scales, \n",
    "#query_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_scales.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_global.to(device),\n",
    "index_local.to(device),\n",
    "index_mask.to(device),\n",
    "index_scales.to(device),\n",
    "index_positions.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in tqdm(range(top_k)):\n",
    "    nnids = eval_nn_inds[:, i]\n",
    "    index_global    = []\n",
    "    for idx in range(nnids.size(dim=0)):\n",
    "        index_global.append(gallery_global[idx, nnids[idx]])\n",
    "    index_global = np.stack(index_global, axis=0)\n",
    "    index_global = torch.from_numpy(index_global)\n",
    "    index_local     = []\n",
    "    for idx in range(nnids.size(dim=0)):\n",
    "        index_local.append(gallery_local[idx, nnids[idx]])\n",
    "    index_local = np.stack(index_local, axis=0)\n",
    "    index_local = torch.from_numpy(index_local)\n",
    "    index_mask     = []\n",
    "    for idx in range(nnids.size(dim=0)):\n",
    "        index_mask.append(gallery_mask[idx, nnids[idx]])\n",
    "    index_mask = np.stack(index_mask, axis=0)\n",
    "    index_mask = torch.from_numpy(index_mask)\n",
    "    index_scales     = []\n",
    "    for idx in range(nnids.size(dim=0)):\n",
    "        index_scales.append(gallery_scales[idx, nnids[idx]])\n",
    "    index_scales = np.stack(index_scales, axis=0)\n",
    "    index_scales = torch.from_numpy(index_scales)\n",
    "    index_positions     = []\n",
    "    for idx in range(nnids.size(dim=0)):\n",
    "        index_positions.append(gallery_positions[idx, nnids[idx]])\n",
    "    index_positions = np.stack(index_positions, axis=0)\n",
    "    index_positions = torch.from_numpy(index_positions)\n",
    "    \n",
    "    index_global = index_global.to(device)\n",
    "    index_local = index_local.to(device)\n",
    "    index_mask = index_mask.to(device)\n",
    "    index_scales = index_scales.to(device)\n",
    "    index_positions = index_positions.to(device)\n",
    "\n",
    "    print(\"index_global size: \", index_global.shape)\n",
    "    print(\"index_local size: \", index_local.shape)\n",
    "    print(\"index_mask size: \", index_mask.shape)\n",
    "    print(\"index_scales size: \", index_scales.shape)\n",
    "    print(\"index_positions size: \", index_positions.shape)\n",
    "    \n",
    "    current_scores = model(\n",
    "        query_global, query_local, query_mask, query_scales, query_positions,\n",
    "        index_global.to(device),\n",
    "        index_local.to(device),\n",
    "        index_mask.to(device),\n",
    "        index_scales.to(device),\n",
    "        index_positions.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_global.shape, index_local.shape, index_mask.shape, index_scales.shape, index_positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery_global[:, nnids].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in tqdm(range(top_k)):\n",
    "    print(\"gallery_global size: \", gallery_global.shape)\n",
    "    print(\"gallery_local size: \", gallery_local.shape)\n",
    "    print(\"gallery_mask size: \", gallery_mask.shape)\n",
    "    print(\"gallery_scales size: \", gallery_scales.shape)\n",
    "    print(\"gallery_positions size: \", gallery_positions.shape)\n",
    "    nnids = eval_nn_inds[:, i]\n",
    "    index_global    = []\n",
    "    for idx in range(nnids.size(dim=0)):\n",
    "        index_global.append(gallery_global[idx, nnids[idx]])\n",
    "    index_global = np.stack(index_global, axis=0)\n",
    "    index_global = torch.from_numpy(index_global)\n",
    "    index_local     = []\n",
    "    for idx in range(nnids.size(dim=0)):\n",
    "        index_local.append(gallery_local[idx, nnids[idx]])\n",
    "    index_local = np.stack(index_local, axis=0)\n",
    "    index_local = torch.from_numpy(index_local)\n",
    "    index_mask     = []\n",
    "    for idx in range(nnids.size(dim=0)):\n",
    "        index_mask.append(gallery_mask[idx, nnids[idx]])\n",
    "    index_mask = np.stack(index_mask, axis=0)\n",
    "    index_mask = torch.from_numpy(index_mask)\n",
    "    index_scales     = []\n",
    "    for idx in range(nnids.size(dim=0)):\n",
    "        index_scales.append(gallery_scales[idx, nnids[idx]])\n",
    "    index_scales = np.stack(index_scales, axis=0)\n",
    "    index_scales = torch.from_numpy(index_scales)\n",
    "    #index_scales    = gallery_scales[:, nnids]\n",
    "    index_positions     = []\n",
    "    for idx in range(nnids.size(dim=0)):\n",
    "        index_positions.append(gallery_positions[idx, nnids[idx]])\n",
    "    index_positions = np.stack(index_positions, axis=0)\n",
    "    index_positions = torch.from_numpy(index_positions)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"index_global size: \", index_global.shape)\n",
    "    print(\"index_local size: \", index_local.shape)\n",
    "    print(\"index_mask size: \", index_mask.shape)\n",
    "    print(\"index_scales size: \", index_scales.shape)\n",
    "    print(\"index_positions size: \", index_positions.shape)\n",
    "    \n",
    "    print(\"index_global device: \", index_global.get_device())\n",
    "    print(\"index_local device: \", index_local.get_device())\n",
    "    print(\"index_mask device: \", index_mask.get_device())\n",
    "    print(\"index_scales device: \", index_scales.get_device())\n",
    "    print(\"index_positions device: \", index_positions.get_device())\n",
    "    \n",
    "    index_global.to(device),\n",
    "    index_local.to(device),\n",
    "    index_mask.to(device),\n",
    "    index_scales.to(device),\n",
    "    index_positions.to(device)\n",
    "\n",
    "    current_scores = model(\n",
    "        query_global, query_local, query_mask, query_scales, query_positions,\n",
    "        index_global.to(device),\n",
    "        index_local.to(device),\n",
    "        index_mask.to(device),\n",
    "        index_scales.to(device),\n",
    "        index_positions.to(device))\n",
    "    scores.append(current_scores.cpu().data)\n",
    "scores = torch.stack(scores, -1) # 70 x 100\n",
    "closest_dists, indices = torch.sort(scores, dim=-1, descending=True)\n",
    "closest_indices = torch.gather(medium_nn_inds, -1, indices)\n",
    "ranks = deepcopy(medium_nn_inds)\n",
    "ranks[:, :top_k] = deepcopy(closest_indices)\n",
    "ranks = ranks.cpu().data.numpy().T\n",
    "# pickle_save('medium_nn_inds.pkl', ranks.T)\n",
    "out = compute_metrics('viquae', ranks, gnd['gnd'], kappas=ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "    scores = []\n",
    "    for i in tqdm(range(top_k)):\n",
    "        nnids = medium_nn_inds[:, i]\n",
    "        index_global    = gallery_global[nnids]\n",
    "        index_local     = gallery_local[nnids]\n",
    "        index_mask      = gallery_mask[nnids]\n",
    "        index_scales    = gallery_scales[nnids]\n",
    "        index_positions = gallery_positions[nnids]\n",
    "        current_scores = model(\n",
    "            query_global, query_local, query_mask, query_scales, query_positions,\n",
    "            index_global.to(device),\n",
    "            index_local.to(device),\n",
    "            index_mask.to(device),\n",
    "            index_scales.to(device),\n",
    "            index_positions.to(device))\n",
    "        scores.append(current_scores.cpu().data)\n",
    "    scores = torch.stack(scores, -1) # 70 x 100\n",
    "    closest_dists, indices = torch.sort(scores, dim=-1, descending=True)\n",
    "    closest_indices = torch.gather(medium_nn_inds, -1, indices)\n",
    "    ranks = deepcopy(medium_nn_inds)\n",
    "    ranks[:, :top_k] = deepcopy(closest_indices)\n",
    "    ranks = ranks.cpu().data.numpy().T\n",
    "    # pickle_save('medium_nn_inds.pkl', ranks.T)\n",
    "    out = compute_metrics('viquae', ranks, gnd['gnd'], kappas=ks)\n",
    "\n",
    "    ########################################################################################\n",
    "    ########################################################################################  \n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_function = partial(evaluate, model=model, \n",
    "        cache_nn_inds=cache_nn_inds,\n",
    "        recall=recall_ks, query_loader=loaders.query, gallery_loader=loaders.gallery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#metrics = eval_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data.delf import datum_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_aqe = False\n",
    "aqe_params = {'k': 2, 'alpha': 0.3}\n",
    "\n",
    "save_nn_inds = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(osp.join(data_dir,  set_name+'_query.txt')) as fid:\n",
    "    query_lines   = fid.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(osp.join(data_dir, set_name+'_gallery.txt')) as fid:\n",
    "    gallery_lines = fid.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_feats = []\n",
    "for i in tqdm(range(len(query_lines))):\n",
    "    name = osp.splitext(osp.basename(query_lines[i].split(';;')[0]))[0]\n",
    "    path = osp.join(data_dir, 'delg_' + feature_name, name + '.delg_global')\n",
    "    query_feats.append(datum_io.ReadFromFile(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_feats = np.stack(query_feats, axis=0)\n",
    "query_feats = query_feats / LA.norm(query_feats, axis=-1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_lines = np.genfromtxt('/mnt/beegfs/home/smessoud/RerankingTransformer/models/research/delf/delf/python/delg/data/viquae_for_rrt/'+\n",
    "                                set_name+'_selection_imgs.txt', dtype='str')\n",
    "selection_lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_lines[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_img   = '.'.join((wiki_item['image'].split('.')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selection_index_feats = []\n",
    "for i in tqdm(range(len(selection_lines))):\n",
    "    index_feats = []\n",
    "    for image_file in selection_lines[i]:\n",
    "        name = '.'.join((image_file.split('.')[:-1]))\n",
    "        path = osp.join(data_dir, 'delg_' + feature_name, name + '.delg_global')\n",
    "        index_feats.append(datum_io.ReadFromFile(path))\n",
    "    selection_index_feats.append(index_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min([len(selection_index_feats[i]) for i in range(len(selection_index_feats))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_index_feats = np.array(selection_index_feats)\n",
    "selection_index_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_feats[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_sims = []\n",
    "for i in range(len(selection_index_feats)):\n",
    "    index_feats = np.stack(selection_index_feats[i], axis=0)\n",
    "    index_feats = index_feats / LA.norm(index_feats, axis=-1)[:, None]\n",
    "    selection_sims.append(np.matmul(query_feats[i], index_feats.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = np.array(selection_sims)\n",
    "sims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_aqe:\n",
    "    ## WARNING: I WAS TOO LAZY TO CORRECT IT\n",
    "    ## IF YOU WANNA USE AQE PARAMATER - ADAPT THE CODE FOR VIQUAE-RRT\n",
    "    alpha = aqe_params['alpha']\n",
    "    nn_inds = np.argsort(-sims, -1)\n",
    "    query_aug = deepcopy(query_feats)\n",
    "    for i in range(len(query_feats)):\n",
    "        new_q = [query_feats[i]]\n",
    "        for j in range(aqe_params['k']):\n",
    "            nn_id = nn_inds[i, j]\n",
    "            weight = sims[i, nn_id] ** aqe_params['alpha']\n",
    "            new_q.append(weight * index_feats[nn_id])\n",
    "        new_q = np.stack(new_q, 0)\n",
    "        new_q = np.mean(new_q, axis=0)\n",
    "        query_aug[i] = new_q/LA.norm(new_q, axis=-1)\n",
    "    sims = np.matmul(query_aug, index_feats.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_index_feats[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_inds = np.argsort(-sims, -1)\n",
    "nn_dists = deepcopy(sims)\n",
    "for i in range(query_feats.shape[0]):\n",
    "    index_feats = selection_index_feats[i]\n",
    "    for j in range(index_feats.shape[0]):\n",
    "        nn_dists[i, j] = sims[i, nn_inds[i, j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_inds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_nn_inds:\n",
    "    output_path = osp.join(data_dir, set_name + '_nn_inds_%s.pkl' % feature_name)\n",
    "    pickle_save(output_path, nn_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnd_data = pickle_load(osp.join(data_dir, gnd_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnd_data['simlist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(dataset, ranks, gnd, kappas=[1, 5, 10]):\n",
    "    print(ranks.shape)\n",
    "    \n",
    "    # old evaluation protocol\n",
    "    if dataset.startswith('classic'):\n",
    "        map, aps, _, _ = compute_map(ranks, gnd)\n",
    "        out = {'map': np.around(map*100, decimals=3)}\n",
    "        print('>> {}: mAP {:.2f}'.format(dataset, out['map']))\n",
    "\n",
    "    # new evaluation protocol\n",
    "    elif dataset.startswith('viquae'):\n",
    "        \n",
    "        gnd_t = []\n",
    "        for i in range(len(gnd)):\n",
    "            g = {}\n",
    "            g['ok'] = np.concatenate([gnd[i]['hard']])\n",
    "            g['junk'] = np.concatenate([gnd[i]['junk']])\n",
    "            gnd_t.append(g)\n",
    "        mapH, apsH, mprH, prsH = compute_map(ranks, gnd_t, kappas)\n",
    "\n",
    "\n",
    "        out = {\n",
    "            'H_map': np.around(mapH*100, decimals=2),\n",
    "            'H_mp':  np.around(mprH*100, decimals=2),\n",
    "        }\n",
    "\n",
    "        print('>> {}: mAP H: {}'.format(dataset, out['H_map']))\n",
    "        print('>> {}: mP@k{} H: {}'.format(dataset, kappas, out['H_mp']))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.arange(nn_inds.T.shape[0])[np.in1d(nn_inds.T[:,i], gnd_data['gnd'][0]['junk'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compute_metrics('viquae', nn_inds.T, gnd_data['gnd'], kappas=[1,5,6,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
